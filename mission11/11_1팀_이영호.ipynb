{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d417a53",
   "metadata": {},
   "source": [
    "# 미션 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f51d5f",
   "metadata": {},
   "source": [
    "기계 번역 실습으로, 한국어 문장을 영어로 번역하는 모델을 구축\n",
    "\n",
    "총 3가지 모델(Seq2Seq 기본 모델, Attention 적용 모델)을 구현하고 학습시키며, 각 모델의 성능을 비교 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a95193",
   "metadata": {},
   "source": [
    "JSON 파일 형식으로 제공되며, 각 항목은 한국어 문장(\"ko\")과 영어 번역문(\"mt\")으로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d20eb",
   "metadata": {},
   "source": [
    "## 가이드라인\n",
    "- 데이터 전처리\n",
    "    - 적절한 토크나이저를 선택하여 한국어, 영어 문장을 토큰화하세요.\n",
    "    - 문장 길이를 분석하여, 전체 문장의 길이에 맞게 최대 길이(MAX_LENGTH)를 설정하고, 필요한 경우 SOS, EOS, PAD, UNK 등의 특수 토큰을 정의한다.\n",
    "- 어휘 사전 구축\n",
    "    - 한국어와 영어 각각의 어후 사전을 구성\n",
    "    - 단어의 등장 빈도를 고려하여, 추후 임베딩이나 기타 모델 구성에 활용할 수 있다.\n",
    "- 텐서 변환 및 데이터 로더 구현\n",
    "    - 각 문장을 토큰의 인덱스 시퀀스로 변환한 후, 고정 길이에 맞게 PAD 토큰으로 패딩한다.\n",
    "    - `TensorDataset`과 `DataLoader`를 활용하여 학습 데이터를 배치 단위로 효율적으로 처리할 수 있도록 구현\n",
    "- 모델 구현 및 실습\n",
    "    - Seq2Seq 모델\n",
    "        - 기본 GRU 기반의 Encoder-Decoder 모델을 구현하고, Teacher Forcing 기법을 적용해 학습한다.\n",
    "    - Attention 모델\n",
    "        - Bahdanau Attention(Bahdanau 혹은 Luong)을 적용한 디코더를 구현하여, 번역 성능을 높이기\n",
    "- 모델 학습 및 평가\n",
    "    - 각 모델별로 학습을 진행한 후, 평가 함수를 활용하여 번역 결과를 확인한다.\n",
    "    - 무작위 문장 쌍에 대해 모델의 변역 결과를 출력하고, 출력 문장을 정제(특수 토큰 제거 등)하여 성능을 분석\n",
    "- 모델 성능 개선 (심화)\n",
    "    - 데이터 전처리 방법 개선(예:불용어 제거, 정규화 등)\n",
    "    - 모델 구조 변경(레이어 수, 은닉 상태 크기, Attention 기법 수정 등)\n",
    "    - 하이퍼파라미터 튜닝(학습률, 배치 크기 등)\n",
    "    - 다양한 평가 지표(예: BLEU 점수) 도입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1364a",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333d03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "Requirement already satisfied: konlpy in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y mecab mecab-ipadic-utf8 libmecab-dev\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335b54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from konlpy.tag import Okt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47439e",
   "metadata": {},
   "source": [
    "## GPU 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379ab93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0\n",
      "MPS available: True\n",
      "CUDA available: False\n",
      "Using MPS (Apple Silicon GPU)\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # 맥북 M1/M2 GPU\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU (Colab, Windows 등)\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75919429",
   "metadata": {},
   "source": [
    "## 한국어 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebde9e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나눔 폰트를 성공적으로 설정했습니다.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 나눔 폰트가 없는 경우를 대비한 대체 폰트 설정\n",
    "try:\n",
    "    # 나눔 폰트가 설치되어 있는지 확인\n",
    "    path = '../NanumGothic.ttf'\n",
    "    font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
    "    plt.rc('font', family=font_name)\n",
    "    fm.fontManager.addfont(path)\n",
    "    print(\"나눔 폰트를 성공적으로 설정했습니다.\")\n",
    "except:\n",
    "    # 나눔 폰트가 없는 경우 기본 폰트 사용\n",
    "    print(\"나눔 폰트를 찾을 수 없습니다. 기본 폰트를 사용합니다.\")\n",
    "    plt.rc('font', family='DejaVu Sans')  # 기본 폰트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fe40f",
   "metadata": {},
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfbc89",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d326a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 데이터 경로: /Users/leeyoungho/develop/ai_study/mission/mission11/nltk_data\n",
      "경로 존재 여부: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leeyoungho/develop/ai_\n",
      "[nltk_data]     study/mission/mission11/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/leeyoungho/develop\n",
      "[nltk_data]     /ai_study/mission/mission11/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 데이터를 현재 폴더의 nltk_data 디렉토리에 다운로드\n",
    "current_dir = os.getcwd()\n",
    "nltk_data_dir = os.path.join(current_dir, 'nltk_data')\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "print(f\"NLTK 데이터 경로: {nltk_data_dir}\")\n",
    "print(f\"경로 존재 여부: {os.path.exists(nltk_data_dir)}\")\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522b868",
   "metadata": {},
   "source": [
    "## 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556c0aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11fe98290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995378d",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad050b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 경로:\n",
      "훈련 데이터: ./일상생활및구어체_한영/일상생활및구어체_한영_train_set.json\n",
      "검증 데이터: ./일상생활및구어체_한영/일상생활및구어체_한영_valid_set.json\n"
     ]
    }
   ],
   "source": [
    "train_json_file_path = \"./일상생활및구어체_한영/일상생활및구어체_한영_train_set.json\"\n",
    "valid_json_file_path = \"./일상생활및구어체_한영/일상생활및구어체_한영_valid_set.json\"\n",
    "\n",
    "# 결과 저장 폴더 생성\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "print(\"데이터 경로:\")\n",
    "print(f\"훈련 데이터: {train_json_file_path}\")\n",
    "print(f\"검증 데이터: {valid_json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f9926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 로딩 중 ===\n",
      "로드된 데이터 개수: 10000\n",
      "로드된 데이터 개수: 1000\n",
      "훈련 데이터: 한국어 10000개, 영어 10000개\n",
      "검증 데이터: 한국어 1000개, 영어 1000개\n"
     ]
    }
   ],
   "source": [
    "def load_json(file_path, max_samples=None):\n",
    "    \"\"\"JSON 파일을 로드하고 데이터 추출\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if max_samples:\n",
    "            data = data[\"data\"][:max_samples]\n",
    "        else:\n",
    "            data = data[\"data\"]\n",
    "            \n",
    "        print(f\"로드된 데이터 개수: {len(data)}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 로딩 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# 데이터 로드\n",
    "print(\"=== 데이터 로딩 중 ===\")\n",
    "data_train = load_json(train_json_file_path, max_samples=10000)\n",
    "data_valid = load_json(valid_json_file_path, max_samples=1000)\n",
    "\n",
    "# ko와 mt 데이터 추출\n",
    "ko_sentences_train = [item[\"ko\"] for item in data_train]\n",
    "mt_sentences_train = [item[\"mt\"] for item in data_train]\n",
    "ko_sentences_valid = [item[\"ko\"] for item in data_valid]\n",
    "mt_sentences_valid = [item[\"mt\"] for item in data_valid]\n",
    "\n",
    "print(f\"훈련 데이터: 한국어 {len(ko_sentences_train)}개, 영어 {len(mt_sentences_train)}개\")\n",
    "print(f\"검증 데이터: 한국어 {len(ko_sentences_valid)}개, 영어 {len(mt_sentences_valid)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "715459f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: ['sn', 'data_set', 'domain', 'subdomain', 'ko_original', 'ko', 'mt', 'en', 'source_language', 'target_language', 'word_count_ko', 'word_count_en', 'word_ratio', 'file_name', 'source', 'license', 'style', 'included_unknown_words', 'ner']\n",
      "Value: {'sn': 'INTSALDSUT062119042703238', 'data_set': '일상생활및구어체', 'domain': '해외영업', 'subdomain': '도소매유통', 'ko_original': '원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.', 'ko': '원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.', 'mt': 'If you reply to the color you want, we will start making it right away.', 'en': 'If you reply to the color you want, we will start making it right away.', 'source_language': 'ko', 'target_language': 'en', 'word_count_ko': 7, 'word_count_en': 15, 'word_ratio': 2.143, 'file_name': 'INTSAL_DSUT.xlsx', 'source': '크라우드소싱', 'license': 'open', 'style': '구어체', 'included_unknown_words': False, 'ner': None}\n",
      "\n",
      "Key: ['sn', 'data_set', 'domain', 'subdomain', 'ko_original', 'ko', 'mt', 'en', 'source_language', 'target_language', 'word_count_ko', 'word_count_en', 'word_ratio', 'file_name', 'source', 'license', 'style', 'included_unknown_words', 'ner']\n",
      "Value: {'sn': 'KTOS062012215138740', 'data_set': '일상생활및구어체', 'domain': '일상생활', 'subdomain': '여행', 'ko_original': '>아, 진짜요?', 'ko': '>아, 진짜요?', 'mt': 'Oh, really?', 'en': '>Oh, really?', 'source_language': 'ko', 'target_language': 'en', 'word_count_ko': 2, 'word_count_en': 2, 'word_ratio': 1.0, 'file_name': '여행_KTOS.xlsx', 'source': 'SBS', 'license': 'open', 'style': '구어체', 'included_unknown_words': False, 'ner': None}\n"
     ]
    }
   ],
   "source": [
    "if len(data_train) > 0:\n",
    "    print(f\"Key: {list(data_train[0].keys())}\")\n",
    "    print(f\"Value: {data_train[0]}\")\n",
    "    print()\n",
    "\n",
    "if len(data_valid) > 0:\n",
    "    print(f\"Key: {list(data_valid[0].keys())}\")\n",
    "    print(f\"Value: {data_valid[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e284dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 샘플 ===\n",
      "샘플 1:\n",
      "  한국어: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "  영어: If you reply to the color you want, we will start making it right away.\n",
      "\n",
      "샘플 2:\n",
      "  한국어: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "  영어: I know what the funniest picture is.\n",
      "\n",
      "샘플 3:\n",
      "  한국어: >속옷을?\n",
      "  영어: Underwear?\n",
      "\n",
      "샘플 4:\n",
      "  한국어: 그래도 가격이 꽤 비싸니까 많이 살게요.\n",
      "  영어: However, the price is quite high, so I will buy a lot.\n",
      "\n",
      "샘플 5:\n",
      "  한국어: AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.\n",
      "  영어: AAA, I really want to apologize for being upset at the meeting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 샘플 확인\n",
    "print(\"=== 데이터 샘플 ===\")\n",
    "for i in range(min(5, len(data_train))):\n",
    "    print(f\"샘플 {i+1}:\")\n",
    "    print(f\"  한국어: {ko_sentences_train[i]}\")\n",
    "    print(f\"  영어: {mt_sentences_train[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b33a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토크나이저 테스트 ===\n",
      "한국어 토큰화: 안녕하세요 오늘 날씨가 좋네요 -> ['안녕하세요', '오늘', '날씨', '가', '좋네요']\n",
      "영어 토큰화: Hello, how are you today? -> ['Hello', ',', 'how', 'are', 'you', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "# 한국어 및 영어 토크나이저 설정\n",
    "tokenizer_ko = Okt().morphs\n",
    "tokenizer_en = word_tokenize\n",
    "\n",
    "# 토크나이저 테스트\n",
    "print(\"=== 토크나이저 테스트 ===\")\n",
    "test_ko = \"안녕하세요 오늘 날씨가 좋네요\"\n",
    "test_en = \"Hello, how are you today?\"\n",
    "\n",
    "print(f\"한국어 토큰화: {test_ko} -> {tokenizer_ko(test_ko)}\")\n",
    "print(f\"영어 토큰화: {test_en} -> {tokenizer_en(test_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07636184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 문장 길이 분석 ===\n",
      "한국어 문장 길이 통계:\n",
      "  최소: 1\n",
      "  최대: 51\n",
      "  평균: 11.27\n",
      "  중간값: 10.00\n",
      "\n",
      "영어 문장 길이 통계:\n",
      "  최소: 1\n",
      "  최대: 51\n",
      "  평균: 11.64\n",
      "  중간값: 11.00\n",
      "\n",
      "설정된 MAX_LENGTH: 25\n",
      "한국어 최대 길이: 51\n",
      "영어 최대 길이: 51\n"
     ]
    }
   ],
   "source": [
    "# 문장 길이 분석\n",
    "print(\"=== 문장 길이 분석 ===\")\n",
    "ko_lengths = [len(tokenizer_ko(sent)) for sent in ko_sentences_train]\n",
    "en_lengths = [len(tokenizer_en(sent)) for sent in mt_sentences_train]\n",
    "\n",
    "print(f\"한국어 문장 길이 통계:\")\n",
    "print(f\"  최소: {min(ko_lengths)}\")\n",
    "print(f\"  최대: {max(ko_lengths)}\")\n",
    "print(f\"  평균: {np.mean(ko_lengths):.2f}\")\n",
    "print(f\"  중간값: {np.median(ko_lengths):.2f}\")\n",
    "\n",
    "print(f\"\\n영어 문장 길이 통계:\")\n",
    "print(f\"  최소: {min(en_lengths)}\")\n",
    "print(f\"  최대: {max(en_lengths)}\")\n",
    "print(f\"  평균: {np.mean(en_lengths):.2f}\")\n",
    "print(f\"  중간값: {np.median(en_lengths):.2f}\")\n",
    "\n",
    "# MAX_LENGTH 설정 (95% 백분위수 기준)\n",
    "ko_95th = np.percentile(ko_lengths, 95)\n",
    "en_95th = np.percentile(en_lengths, 95)\n",
    "MAX_LENGTH = int(max(ko_95th, en_95th)) + 2  # SOS, EOS 토큰 포함\n",
    "#MAX_LENGTH = max(max(ko_lengths), max(en_lengths)) + 2 \n",
    "\n",
    "print(f\"\\n설정된 MAX_LENGTH: {MAX_LENGTH}\")\n",
    "print(f\"한국어 최대 길이: {max(ko_lengths)}\")\n",
    "print(f\"영어 최대 길이: {max(en_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493d7d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 특수 토큰 정의 ===\n",
      "SOS_token: 0\n",
      "EOS_token: 1\n",
      "PAD_token: 2\n",
      "UNK_token: 3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 정의\n",
    "SOS_token = 0  # Start of Sentence\n",
    "EOS_token = 1  # End of Sentence\n",
    "PAD_token = 2  # Padding\n",
    "UNK_token = 3  # Unknown\n",
    "\n",
    "print(\"=== 특수 토큰 정의 ===\")\n",
    "print(f\"SOS_token: {SOS_token}\")\n",
    "print(f\"EOS_token: {EOS_token}\")\n",
    "print(f\"PAD_token: {PAD_token}\")\n",
    "print(f\"UNK_token: {UNK_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11e29d",
   "metadata": {},
   "source": [
    "# 어휘 사전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485e64",
   "metadata": {},
   "source": [
    "## Lang 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f029f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lang 클래스 테스트 ===\n",
      "어휘 크기: 6\n",
      "단어-인덱스: {2: 'PAD', 0: 'SOS', 1: 'EOS', 3: '<unk>', '안녕하세요': 4, '반갑습니다': 5}\n",
      "인덱스-단어: {2: 'PAD', 0: 'SOS', 1: 'EOS', 3: '<unk>', 4: '안녕하세요', 5: '반갑습니다'}\n",
      "단어 빈도수: {2: 0, 0: 0, 1: 0, 3: 0, '안녕하세요': 1, '반갑습니다': 1}\n"
     ]
    }
   ],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # 초기에는 PAD, SOS, EOS, UNK 토큰을 미리 등록\n",
    "        self.word2index = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
    "        # word2count도 특수 토큰으로 초기화\n",
    "        self.word2count = {PAD_token: 0, SOS_token: 0, EOS_token: 0, UNK_token: 0}\n",
    "        self.n_words = 4  # PAD, SOS, EOS, UNK 포함\n",
    "\n",
    "    def addSentence(self, sentence, tokenizer):\n",
    "        \"\"\"문장을 토큰화하여 어휘 사전에 추가\"\"\"\n",
    "        for word in tokenizer(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"단어를 어휘 사전에 추가\"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"어휘 크기 반환\"\"\"\n",
    "        return self.n_words\n",
    "\n",
    "    def get_word_frequency(self, min_freq=1):\n",
    "        \"\"\"최소 빈도수 이상의 단어만 반환\"\"\"\n",
    "        return {word: count for word, count in self.word2count.items() if count >= min_freq}\n",
    "\n",
    "# Lang 클래스 테스트\n",
    "print(\"=== Lang 클래스 테스트 ===\")\n",
    "test_lang = Lang(\"test\")\n",
    "test_sentence = \"안녕하세요 반갑습니다\"\n",
    "test_lang.addSentence(test_sentence, tokenizer_ko)\n",
    "\n",
    "print(f\"어휘 크기: {test_lang.get_vocab_size()}\")\n",
    "print(f\"단어-인덱스: {test_lang.word2index}\")\n",
    "print(f\"인덱스-단어: {test_lang.index2word}\")\n",
    "print(f\"단어 빈도수: {test_lang.word2count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654968d",
   "metadata": {},
   "source": [
    "## 어휘 사전 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c068ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 어휘 사전 구축 중 ===\n",
      "한국어 문장 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "한국어 처리: 100%|██████████| 10000/10000 [00:07<00:00, 1282.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 문장 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "영어 처리: 100%|██████████| 10000/10000 [00:00<00:00, 26721.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 어휘 통계 ===\n",
      "한국어 어휘 크기: 13775\n",
      "영어 어휘 크기: 9942\n",
      "\n",
      "한국어 상위 10개 단어: [('.', 8520), ('을', 2868), ('이', 2532), ('>', 2398), (',', 2325), ('에', 2082), ('를', 1585), ('의', 1519), ('가', 1481), ('는', 1269)]\n",
      "영어 상위 10개 단어: [('.', 8643), (',', 4518), ('the', 3815), ('to', 2839), ('I', 2490), ('you', 2371), ('a', 2272), ('and', 1850), ('>', 1823), ('it', 1696)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, tokenizer1, tokenizer2, min_freq=2):\n",
    "    \"\"\"한국어와 영어 어휘 사전 구축\"\"\"\n",
    "    print(\"=== 어휘 사전 구축 중 ===\")\n",
    "    \n",
    "    input_lang = Lang(lang1)  # 한국어\n",
    "    output_lang = Lang(lang2)  # 영어\n",
    "    \n",
    "    print(\"한국어 문장 처리 중...\")\n",
    "    for sentence in tqdm(ko_sentences_train, desc=\"한국어 처리\"):\n",
    "        input_lang.addSentence(sentence, tokenizer1)\n",
    "    \n",
    "    print(\"영어 문장 처리 중...\")\n",
    "    for sentence in tqdm(mt_sentences_train, desc=\"영어 처리\"):\n",
    "        output_lang.addSentence(sentence, tokenizer2)\n",
    "    \n",
    "    # 최소 빈도수 필터링\n",
    "    print(f\"\\n=== 어휘 통계 ===\")\n",
    "    print(f\"한국어 어휘 크기: {input_lang.get_vocab_size()}\")\n",
    "    print(f\"영어 어휘 크기: {output_lang.get_vocab_size()}\")\n",
    "    \n",
    "    # 빈도수 상위 10개 단어 확인 (특수 토큰 제외)\n",
    "    ko_top_words = [(word, count) for word, count in input_lang.word2count.items() \n",
    "                    if word not in [\"PAD\", \"SOS\", \"EOS\", \"<unk>\"]]\n",
    "    ko_top_words = sorted(ko_top_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    en_top_words = [(word, count) for word, count in output_lang.word2count.items() \n",
    "                    if word not in [\"PAD\", \"SOS\", \"EOS\", \"<unk>\"]]\n",
    "    en_top_words = sorted(en_top_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\n한국어 상위 10개 단어: {ko_top_words}\")\n",
    "    print(f\"영어 상위 10개 단어: {en_top_words}\")\n",
    "    \n",
    "    return input_lang, output_lang\n",
    "\n",
    "# 어휘 사전 구축\n",
    "input_lang, output_lang = prepareData(\"한국어\", \"영어\", tokenizer_ko, tokenizer_en, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e44b1",
   "metadata": {},
   "source": [
    "## 어휘 사전 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a25067f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전 저장 완료: models/korean_vocab.pkl\n",
      "어휘 사전 저장 완료: models/english_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# 어휘 사전 저장 (나중에 재사용하기 위해)\n",
    "def save_vocab(lang, filename):\n",
    "    \"\"\"어휘 사전을 파일로 저장\"\"\"\n",
    "    vocab_data = {\n",
    "        'word2index': lang.word2index,\n",
    "        'index2word': lang.index2word,\n",
    "        'word2count': lang.word2count,\n",
    "        'n_words': lang.n_words\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(vocab_data, f)\n",
    "    print(f\"어휘 사전 저장 완료: {filename}\")\n",
    "\n",
    "def load_vocab(filename):\n",
    "    \"\"\"저장된 어휘 사전을 파일에서 로드\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "    \n",
    "    lang = Lang(\"loaded\")\n",
    "    lang.word2index = vocab_data['word2index']\n",
    "    lang.index2word = vocab_data['index2word']\n",
    "    lang.word2count = vocab_data['word2count']\n",
    "    lang.n_words = vocab_data['n_words']\n",
    "    \n",
    "    print(f\"어휘 사전 로드 완료: {filename}\")\n",
    "    return lang\n",
    "\n",
    "# 어휘 사전 저장\n",
    "save_vocab(input_lang, \"models/korean_vocab.pkl\")\n",
    "save_vocab(output_lang, \"models/english_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf3d6f",
   "metadata": {},
   "source": [
    "# 데이터 전처리 및 텐서 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12b74a",
   "metadata": {},
   "source": [
    "## s2i, i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5094164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 변환 함수 테스트 ===\n",
      "원본: 안녕하세요 반갑습니다\n",
      "토큰화: ['안녕하세요', '반갑습니다']\n",
      "인덱스: [0, 211, 5187, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "재구성: 안녕하세요 반갑습니다\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_indexes(sentence, lang, tokenizer, max_length):\n",
    "    \"\"\"문장을 토큰 인덱스로 변환\"\"\"\n",
    "    tokens = tokenizer(sentence)\n",
    "    indexes = [lang.word2index.get(token, UNK_token) for token in tokens]\n",
    "    \n",
    "    # SOS, EOS 토큰 추가\n",
    "    indexes = [SOS_token] + indexes + [EOS_token]\n",
    "    \n",
    "    # 패딩\n",
    "    if len(indexes) < max_length:\n",
    "        indexes += [PAD_token] * (max_length - len(indexes))\n",
    "    else:\n",
    "        indexes = indexes[:max_length-1] + [EOS_token]\n",
    "    \n",
    "    return indexes\n",
    "\n",
    "def indexes_to_sentence(indexes, lang):\n",
    "    \"\"\"인덱스를 문장으로 변환\"\"\"\n",
    "    words = []\n",
    "    for idx in indexes:\n",
    "        if idx == PAD_token:\n",
    "            continue\n",
    "        if idx == EOS_token:\n",
    "            break\n",
    "        if idx in [SOS_token, PAD_token]:\n",
    "            continue\n",
    "        words.append(lang.index2word.get(idx, \"<unk>\"))\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 변환 함수 테스트\n",
    "print(\"=== 변환 함수 테스트 ===\")\n",
    "test_ko = \"안녕하세요 반갑습니다\"\n",
    "test_indexes = sentence_to_indexes(test_ko, input_lang, tokenizer_ko, MAX_LENGTH)\n",
    "test_reconstructed = indexes_to_sentence(test_indexes, input_lang)\n",
    "\n",
    "print(f\"원본: {test_ko}\")\n",
    "print(f\"토큰화: {tokenizer_ko(test_ko)}\")\n",
    "print(f\"인덱스: {test_indexes}\")\n",
    "print(f\"재구성: {test_reconstructed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803c836",
   "metadata": {},
   "source": [
    "## 데이터셋 및 Dataloader 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058bd080",
   "metadata": {},
   "source": [
    "### case 1 : TranslationDataset 클래스 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17158723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset:\n",
    "    def __init__(self, ko_sentences, en_sentences, input_lang, output_lang, \n",
    "                 tokenizer_ko, tokenizer_en, max_length):\n",
    "        self.ko_sentences = ko_sentences\n",
    "        self.en_sentences = en_sentences\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.tokenizer_ko = tokenizer_ko\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ko_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ko_sentence = self.ko_sentences[idx]\n",
    "        en_sentence = self.en_sentences[idx]\n",
    "        \n",
    "        # 한국어와 영어 문장을 인덱스로 변환\n",
    "        ko_indexes = sentence_to_indexes(ko_sentence, self.input_lang, \n",
    "                                       self.tokenizer_ko, self.max_length)\n",
    "        en_indexes = sentence_to_indexes(en_sentence, self.output_lang, \n",
    "                                       self.tokenizer_en, self.max_length)\n",
    "        \n",
    "        return {\n",
    "            'ko': torch.tensor(ko_indexes, dtype=torch.long),\n",
    "            'en': torch.tensor(en_indexes, dtype=torch.long),\n",
    "            'ko_length': len(self.tokenizer_ko(ko_sentence)),\n",
    "            'en_length': len(self.tokenizer_en(en_sentence))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae29c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터셋 생성 중 ===\n",
      "훈련 데이터셋 크기: 10000\n",
      "검증 데이터셋 크기: 1000\n",
      "\n",
      "=== 데이터셋 테스트 ===\n",
      "샘플 데이터:\n",
      "  한국어 텐서: tensor([ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  1,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2])\n",
      "  영어 텐서: tensor([ 0,  4,  5,  6,  7,  8,  9,  5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "         1,  2,  2,  2,  2,  2,  2])\n",
      "  한국어 길이: 11\n",
      "  영어 길이: 17\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "print(\"=== 데이터셋 생성 중 ===\")\n",
    "train_dataset = TranslationDataset(\n",
    "    ko_sentences_train, mt_sentences_train, \n",
    "    input_lang, output_lang, \n",
    "    tokenizer_ko, tokenizer_en, MAX_LENGTH\n",
    ")\n",
    "\n",
    "valid_dataset = TranslationDataset(\n",
    "    ko_sentences_valid, mt_sentences_valid, \n",
    "    input_lang, output_lang, \n",
    "    tokenizer_ko, tokenizer_en, MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "print(f\"검증 데이터셋 크기: {len(valid_dataset)}\")\n",
    "\n",
    "# 데이터셋 테스트\n",
    "print(\"\\n=== 데이터셋 테스트 ===\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"샘플 데이터:\")\n",
    "print(f\"  한국어 텐서: {sample['ko']}\")\n",
    "print(f\"  영어 텐서: {sample['en']}\")\n",
    "print(f\"  한국어 길이: {sample['ko_length']}\")\n",
    "print(f\"  영어 길이: {sample['en_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3499039",
   "metadata": {},
   "source": [
    "### case 2 : 단순화된 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556743c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 단순화\n",
    "def tensorFromSentence(lang, sentence, tokenizer):\n",
    "    \"\"\"문장을 텐서로 변환하는 함수\"\"\"\n",
    "    indexes = [SOS_token]\n",
    "    indexes += [lang.word2index.get(word, UNK_token) for word in tokenizer(sentence)[:MAX_LENGTH - 2]]\n",
    "    indexes.append(EOS_token)\n",
    "    \n",
    "    # 길이 MAX_LENGTH에 맞춰 PAD 추가\n",
    "    while len(indexes) < MAX_LENGTH:\n",
    "        indexes.append(PAD_token)\n",
    "    \n",
    "    return torch.tensor(indexes[:MAX_LENGTH], dtype=torch.long)\n",
    "\n",
    "def get_dataloader(ko_sentences, en_sentences, batch_size):\n",
    "    \"\"\"데이터 로더 생성 함수\"\"\"\n",
    "    input_tensors = [tensorFromSentence(input_lang, inp, tokenizer_ko) for inp in ko_sentences]\n",
    "    target_tensors = [tensorFromSentence(output_lang, tgt, tokenizer_en) for tgt in en_sentences]\n",
    "\n",
    "    input_tensors = torch.stack(input_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
    "    target_tensors = torch.stack(target_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
    "\n",
    "    dataset = TensorDataset(input_tensors, target_tensors)\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    print(f\"input_tensors.shape: {input_tensors.shape}, target_tensors.shape: {target_tensors.shape}\")\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82f7a8",
   "metadata": {},
   "source": [
    "## DataLoader 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df824bee",
   "metadata": {},
   "source": [
    "### case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DataLoader 생성 완료 ===\n",
      "훈련 배치 수: 313\n",
      "검증 배치 수: 32\n",
      "\n",
      "=== DataLoader 테스트 ===\n",
      "배치 크기:\n",
      "  한국어: torch.Size([32, 25])\n",
      "  영어: torch.Size([32, 25])\n",
      "  한국어 길이: tensor([ 8, 20,  5, 13,  9])...\n",
      "  영어 길이: tensor([10, 19,  5, 10, 12])...\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 생성\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"배치 데이터를 정리하는 함수\"\"\"\n",
    "    ko = torch.stack([b['ko'] for b in batch])      # [B, S]\n",
    "    en = torch.stack([b['en'] for b in batch])      # [B, T]\n",
    "    ko_raw = torch.tensor([b['ko_length'] for b in batch], dtype=torch.long)\n",
    "    en_raw = torch.tensor([b['en_length'] for b in batch], dtype=torch.long)\n",
    "\n",
    "    # BOS/EOS(+2) 반영 후 실제 시퀀스 길이로 clamp\n",
    "    ko_lengths = torch.clamp(ko_raw + 2, max=ko.size(1))\n",
    "    en_lengths = torch.clamp(en_raw + 2, max=en.size(1))\n",
    "\n",
    "    return {'ko': ko, 'en': en, 'ko_lengths': ko_lengths, 'en_lengths': en_lengths}\n",
    "\n",
    "\n",
    "# 훈련 및 검증 DataLoader 생성\n",
    "train_loader_case1 = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0  # Windows에서는 0으로 설정\n",
    ")\n",
    "\n",
    "valid_loader_case1 = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"=== DataLoader 생성 완료 ===\")\n",
    "print(f\"훈련 배치 수: {len(train_loader_case1)}\")\n",
    "print(f\"검증 배치 수: {len(valid_loader_case1)}\")\n",
    "\n",
    "# DataLoader 테스트\n",
    "print(\"\\n=== DataLoader 테스트 ===\")\n",
    "for batch in train_loader_case1:\n",
    "    print(f\"배치 크기:\")\n",
    "    print(f\"  한국어: {batch['ko'].shape}\")\n",
    "    print(f\"  영어: {batch['en'].shape}\")\n",
    "    print(f\"  한국어 길이: {batch['ko_lengths'][:5]}...\")  # 처음 5개만\n",
    "    print(f\"  영어 길이: {batch['en_lengths'][:5]}...\")    # 처음 5개만\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675905f",
   "metadata": {},
   "source": [
    "### case2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3281a1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터셋 생성 중 ===\n",
      "input_tensors.shape: torch.Size([10000, 25]), target_tensors.shape: torch.Size([10000, 25])\n",
      "input_tensors.shape: torch.Size([1000, 25]), target_tensors.shape: torch.Size([1000, 25])\n",
      "훈련 배치 수: 313\n",
      "검증 배치 수: 32\n",
      "\n",
      "=== 데이터셋 테스트 ===\n",
      "배치 크기:\n",
      "  입력: torch.Size([32, 25])\n",
      "  타겟: torch.Size([32, 25])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "print(\"=== 데이터셋 생성 중 ===\")\n",
    "train_loader_case2 = get_dataloader(ko_sentences_train, mt_sentences_train, BATCH_SIZE)\n",
    "valid_loader_case2 = get_dataloader(ko_sentences_valid, mt_sentences_valid, BATCH_SIZE)\n",
    "\n",
    "print(f\"훈련 배치 수: {len(train_loader_case2)}\")\n",
    "print(f\"검증 배치 수: {len(valid_loader_case2)}\")\n",
    "\n",
    "# 데이터셋 테스트\n",
    "print(\"\\n=== 데이터셋 테스트 ===\")\n",
    "for batch in train_loader_case2:\n",
    "    input_tensor, target_tensor = batch\n",
    "    print(f\"배치 크기:\")\n",
    "    print(f\"  입력: {input_tensor.shape}\")\n",
    "    print(f\"  타겟: {target_tensor.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19893678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case1과 Case2 중 하나만 활성화\n",
    "USE_CASE1 = False\n",
    "\n",
    "if USE_CASE1:\n",
    "    # TranslationDataset 사용\n",
    "    train_loader = train_loader_case1\n",
    "    valid_loader = valid_loader_case1\n",
    "else:\n",
    "    # 단순화된 방식 사용\n",
    "    train_loader = train_loader_case2\n",
    "    valid_loader = valid_loader_case2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5358935",
   "metadata": {},
   "source": [
    "# Seq2Seq 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f3edb",
   "metadata": {},
   "source": [
    "## Encoder 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca1eabd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Encoder 테스트 ===\n",
      "입력 크기: torch.Size([2, 10])\n",
      "출력 크기: torch.Size([2, 8, 256])\n",
      "은닉 상태 크기: torch.Size([2, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=PAD_token)\n",
    "        \n",
    "        # GRU 레이어\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_size, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 드롭아웃\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # 임베딩 적용\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        \n",
    "        # PackedSequence로 변환 (패딩 제거)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, input_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # GRU 순전파\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # PackedSequence를 다시 일반 텐서로 변환\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \"\"\"은닉 상태 초기화\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# Encoder 테스트\n",
    "print(\"=== Encoder 테스트 ===\")\n",
    "encoder = EncoderRNN(\n",
    "    input_size=input_lang.get_vocab_size(),\n",
    "    hidden_size=256,\n",
    "    embedding_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# 테스트 입력\n",
    "test_batch_size = 2\n",
    "test_seq_len = 10\n",
    "test_input = torch.randint(0, 100, (test_batch_size, test_seq_len)).to(device)\n",
    "test_lengths = [8, 6]  # 첫 번째는 길이 8, 두 번째는 길이 6\n",
    "\n",
    "# Encoder 순전파\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    outputs, hidden = encoder(test_input, test_lengths)\n",
    "    \n",
    "print(f\"입력 크기: {test_input.shape}\")\n",
    "print(f\"출력 크기: {outputs.shape}\")\n",
    "print(f\"은닉 상태 크기: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7dd14",
   "metadata": {},
   "source": [
    "## Attention Decoder 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b69429c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d854193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0.1, pad_idx=None, device=\"cpu\"):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # 임베딩\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size, padding_idx=pad_idx)\n",
    "\n",
    "        # Attention 레이어들\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        self.attention_combine = nn.Linear(embedding_size + hidden_size, hidden_size)\n",
    "\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # 출력층\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq, hidden, encoder_outputs):\n",
    "        B = input_seq.size(0)\n",
    "        \n",
    "        # 1) 임베딩\n",
    "        embedded = self.dropout(self.embedding(input_seq))  # (B,1,E)\n",
    "        \n",
    "        # 2) Attention 가중치 계산 - 수정된 버전\n",
    "        attention_scores = self.calculate_attention(hidden, encoder_outputs)  # (B,S)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (B,S)\n",
    "        \n",
    "        # 3) Context vector 계산\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (B,1,H)\n",
    "        \n",
    "        # 4) 임베딩과 컨텍스트 결합\n",
    "        gru_input = torch.cat((embedded, context), dim=2)  # (B,1,E+H)\n",
    "        gru_input = self.attention_combine(gru_input)      # (B,1,H)\n",
    "        \n",
    "        # 5) GRU 진행\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        \n",
    "        # 6) 어휘 분포\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, hidden, attention_weights.unsqueeze(1)\n",
    "\n",
    "    def calculate_attention(self, hidden, encoder_outputs):\n",
    "        \"\"\"수정된 Attention 계산\"\"\"\n",
    "        B, S, H = encoder_outputs.size()\n",
    "        \n",
    "        # hidden의 마지막 레이어만 사용\n",
    "        if hidden.dim() == 3:  # (num_layers, B, H)\n",
    "            dec_h_t = hidden[-1]  # (B,H)\n",
    "        else:  # (B, H)\n",
    "            dec_h_t = hidden\n",
    "            \n",
    "        # Attention 스코어 계산\n",
    "        dec_h_expanded = dec_h_t.unsqueeze(1).expand(-1, S, -1)  # (B,S,H)\n",
    "        concat_input = torch.cat([dec_h_expanded, encoder_outputs], dim=2)  # (B,S,2H)\n",
    "        scores = self.attention(concat_input).squeeze(-1)  # (B,S)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7208ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Attention Decoder 재생성 (최종 수정된 버전) ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Attention Decoder 재생성 (최종 수정된 버전) ===\")\n",
    "decoder = AttentionDecoderRNN(\n",
    "    hidden_size=256,\n",
    "    output_size=output_lang.get_vocab_size(),\n",
    "    embedding_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    pad_idx=PAD_token,\n",
    "    device=device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e55e6",
   "metadata": {},
   "source": [
    "## Seq2Seq 모델 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af3202b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, ko_input, ko_lengths, en_input, teacher_forcing_ratio=0.5):\n",
    "        batch_size = ko_input.size(0)\n",
    "        max_length = en_input.size(1)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # 인코더 순전파\n",
    "        encoder_outputs, encoder_hidden = self.encoder(ko_input, ko_lengths)\n",
    "        \n",
    "        # 디코더 초기화\n",
    "        decoder_input = torch.full((batch_size, 1), SOS_token, dtype=torch.long, device=self.device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # 출력 저장용\n",
    "        outputs = torch.zeros(batch_size, max_length, vocab_size, device=self.device)\n",
    "        \n",
    "        # Teacher Forcing 적용\n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            # 디코더 순전파\n",
    "            output, decoder_hidden, attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # 출력 저장\n",
    "            outputs[:, t, :] = output[:, 0, :]\n",
    "            \n",
    "            # Teacher Forcing 비율을 점진적으로 감소\n",
    "            if use_teacher_forcing and t < max_length - 1:\n",
    "                decoder_input = en_input[:, t:t+1]\n",
    "            else:\n",
    "                top1 = output[:, 0, :].argmax(1)\n",
    "                decoder_input = top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, ko_input, ko_lengths, max_length=50):\n",
    "        \"\"\"추론 시 사용하는 함수\"\"\"\n",
    "        batch_size = ko_input.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # 인코더 순전파\n",
    "        encoder_outputs, encoder_hidden = self.encoder(ko_input, ko_lengths)\n",
    "        \n",
    "        # 디코더 초기화\n",
    "        decoder_input = torch.full((batch_size, 1), SOS_token, dtype=torch.long, device=self.device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # 출력 저장용\n",
    "        outputs = torch.zeros(batch_size, max_length, vocab_size, device=self.device)\n",
    "        predicted_indices = torch.zeros(batch_size, max_length, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            # 디코더 순전파\n",
    "            output, decoder_hidden, attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # 출력 저장\n",
    "            outputs[:, t, :] = output[:, 0, :]\n",
    "            \n",
    "            # 다음 토큰 예측\n",
    "            top1 = output[:, 0, :].argmax(1)\n",
    "            predicted_indices[:, t] = top1\n",
    "            decoder_input = top1.unsqueeze(1)\n",
    "            \n",
    "            # EOS 토큰이 나오면 중단\n",
    "            if (top1 == EOS_token).all():\n",
    "                break\n",
    "        \n",
    "        return predicted_indices, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b45db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    \"\"\"모델 가중치 초기화\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "        elif 'bias' in name:\n",
    "            nn.init.constant_(param, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39b32f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Seq2Seq 모델 생성 ===\n",
      "모델 파라미터 수:\n",
      "  총 파라미터: 7,170,647\n",
      "  학습 가능한 파라미터: 7,170,647\n"
     ]
    }
   ],
   "source": [
    "# Seq2Seq 모델 생성\n",
    "print(\"=== Seq2Seq 모델 생성 ===\")\n",
    "#seq2seq_model = Seq2SeqModel(encoder, decoder, device).to(device)\n",
    "seq2seq_model = Seq2SeqModel(encoder, decoder, device).to(device)\n",
    "init_weights(seq2seq_model)\n",
    "\n",
    "print(f\"모델 파라미터 수:\")\n",
    "total_params = sum(p.numel() for p in seq2seq_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in seq2seq_model.parameters() if p.requires_grad)\n",
    "print(f\"  총 파라미터: {total_params:,}\")\n",
    "print(f\"  학습 가능한 파라미터: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775df18d",
   "metadata": {},
   "source": [
    "## 손실 함수 및 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b2edca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 학습 설정 완료 ===\n",
      "손실 함수: CrossEntropyLoss()\n",
      "옵티마이저: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "학습률: 0.001\n"
     ]
    }
   ],
   "source": [
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
    "optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "\n",
    "\n",
    "# Teacher Forcing 비율을 점진적으로 감소\n",
    "def get_teacher_forcing_ratio(epoch, max_epochs):\n",
    "    \"\"\"에포치에 따라 Teacher Forcing 비율을 점진적으로 감소\"\"\"\n",
    "    start_ratio = 0.9\n",
    "    end_ratio = 0.1\n",
    "    decay_rate = (start_ratio - end_ratio) / max_epochs\n",
    "    return max(start_ratio - decay_rate * epoch, end_ratio)\n",
    "\n",
    "\n",
    "print(\"=== 학습 설정 완료 ===\")\n",
    "print(f\"손실 함수: {criterion}\")\n",
    "print(f\"옵티마이저: {optimizer}\")\n",
    "print(f\"학습률: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4500c01",
   "metadata": {},
   "source": [
    "## 학습 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fead5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, teacher_forcing_ratio=0.5):\n",
    "    \"\"\"한 에포크 학습\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # 데이터를 디바이스로 이동\n",
    "        ko_input = batch['ko'].to(device)\n",
    "        en_input = batch['en'].to(device)\n",
    "        ko_lengths = batch['ko_lengths']\n",
    "        en_lengths = batch['en_lengths']\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 순전파\n",
    "        outputs = model(ko_input, ko_lengths, en_input, teacher_forcing_ratio)\n",
    "        \n",
    "        # 손실 계산 - PAD 토큰 제외\n",
    "        batch_size, seq_len, vocab_size = outputs.size()\n",
    "        \n",
    "        # PAD 토큰이 아닌 위치만 마스킹\n",
    "        mask = (en_input != PAD_token).float()  # (B,S)\n",
    "        \n",
    "        # 마스킹된 손실 계산\n",
    "        outputs_flat = outputs.view(-1, vocab_size)\n",
    "        targets_flat = en_input.view(-1)\n",
    "        mask_flat = mask.view(-1)\n",
    "        \n",
    "        # PAD가 아닌 위치만 손실 계산\n",
    "        valid_positions = mask_flat > 0\n",
    "        if valid_positions.sum() > 0:\n",
    "            loss = criterion(\n",
    "                outputs_flat[valid_positions], \n",
    "                targets_flat[valid_positions]\n",
    "            )\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 손실 누적\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 진행률 업데이트\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"한 에포크 검증\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # 데이터를 디바이스로 이동\n",
    "            ko_input = batch['ko'].to(device)\n",
    "            en_input = batch['en'].to(device)\n",
    "            ko_lengths = batch['ko_lengths']\n",
    "            en_lengths = batch['en_lengths']\n",
    "            \n",
    "            # 순전파 (Teacher Forcing 없음)\n",
    "            outputs = model(ko_input, ko_lengths, en_input, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # 손실 계산 - PAD 토큰 제외\n",
    "            batch_size, seq_len, vocab_size = outputs.size()\n",
    "            \n",
    "            # PAD 토큰이 아닌 위치만 마스킹\n",
    "            mask = (en_input != PAD_token).float()  # (B,S)\n",
    "            \n",
    "            # 마스킹된 손실 계산\n",
    "            outputs_flat = outputs.view(-1, vocab_size)\n",
    "            targets_flat = en_input.view(-1)\n",
    "            mask_flat = mask.view(-1)\n",
    "            \n",
    "            # PAD가 아닌 위치만 손실 계산\n",
    "            valid_positions = mask_flat > 0\n",
    "            if valid_positions.sum() > 0:\n",
    "                loss = criterion(\n",
    "                    outputs_flat[valid_positions], \n",
    "                    targets_flat[valid_positions]\n",
    "                )\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            # 손실 누적\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 진행률 업데이트\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "            })\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7cfeb",
   "metadata": {},
   "source": [
    "case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", position=0, leave=True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # 배치 데이터 언패킹 (튜플 형태)\n",
    "        ko_input, en_input = batch\n",
    "        \n",
    "        # 데이터를 디바이스로 이동\n",
    "        ko_input = ko_input.to(device)\n",
    "        en_input = en_input.to(device)\n",
    "        \n",
    "        # 입력 길이 계산 (PAD 토큰 제외) - 명시적으로 CPU로 이동\n",
    "        ko_lengths = (ko_input != PAD_token).sum(dim=1)\n",
    "        ko_lengths = ko_lengths.detach().cpu().long()\n",
    "        \n",
    "        en_lengths = (en_input != PAD_token).sum(dim=1)\n",
    "        en_lengths = en_lengths.detach().cpu().long()\n",
    "        \n",
    "        \n",
    "        #print(f\"ko_input shape: {ko_input.shape}\")\n",
    "        #print(f\"en_input shape: {en_input.shape}\")\n",
    "        #print(f\"ko_lengths shape: {ko_lengths.shape}\")\n",
    "        #print(f\"en_lengths shape: {en_lengths.shape}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Teacher Forcing 적용\n",
    "        if random.random() < teacher_forcing_ratio:\n",
    "            # Teacher Forcing: 실제 정답을 다음 입력으로 사용\n",
    "            outputs = model(ko_input, ko_lengths, en_input)\n",
    "            \n",
    "            #print(f\"outputs shape: {outputs.shape}\")\n",
    "            \n",
    "            # 배치 크기 맞추기: outputs와 target의 크기를 일치시킴\n",
    "            target = en_input[:, 1:].contiguous()  # 첫 번째 토큰 제거\n",
    "            outputs = outputs[:, 1:, :].contiguous()  # outputs에서도 첫 번째 토큰 제거\n",
    "            \n",
    "            target_flat = target.view(-1)\n",
    "            outputs_flat = outputs.view(-1, output_lang.n_words)\n",
    "            \n",
    "            #print(f\"target shape: {target.shape}\")\n",
    "            #print(f\"target_flat shape: {target_flat.shape}\")\n",
    "            #print(f\"outputs_flat shape: {outputs_flat.shape}\")\n",
    "            \n",
    "            loss = criterion(outputs_flat, target_flat)\n",
    "        else:\n",
    "            # No Teacher Forcing: 모델 예측을 다음 입력으로 사용\n",
    "            outputs = model(ko_input, ko_lengths, en_input)\n",
    "            \n",
    "            #print(f\"outputs shape: {outputs.shape}\")\n",
    "            \n",
    "            # 배치 크기 맞추기\n",
    "            target = en_input[:, 1:].contiguous()  # 첫 번째 토큰 제거\n",
    "            outputs = outputs[:, 1:, :].contiguous()  # outputs에서도 첫 번째 토큰 제거\n",
    "            \n",
    "            target_flat = target.view(-1)\n",
    "            outputs_flat = outputs.view(-1, output_lang.n_words)\n",
    "            \n",
    "            #print(f\"target shape: {target.shape}\")\n",
    "            #print(f\"target_flat shape: {target_flat.shape}\")\n",
    "            #print(f\"outputs_flat shape: {outputs_flat.shape}\")\n",
    "            \n",
    "            loss = criterion(outputs_flat, target_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "           \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # 배치 데이터 언패킹\n",
    "            ko_input, en_input = batch\n",
    "            \n",
    "            # 데이터를 디바이스로 이동\n",
    "            ko_input = ko_input.to(device)\n",
    "            en_input = en_input.to(device)\n",
    "            \n",
    "            # 입력 길이 계산 - 명시적으로 CPU로 이동\n",
    "            ko_lengths = (ko_input != PAD_token).sum(dim=1)\n",
    "            ko_lengths = ko_lengths.detach().cpu().long()  # 더 명확한 CPU 이동\n",
    "            \n",
    "            # 검증 시에는 Teacher Forcing 사용하지 않음\n",
    "            outputs = model(ko_input, ko_lengths, en_input)\n",
    "            \n",
    "            #print(f\"validation outputs shape: {outputs.shape}\")\n",
    "            \n",
    "            # 배치 크기 맞추기\n",
    "            target = en_input[:, 1:].contiguous()  # 첫 번째 토큰 제거\n",
    "            outputs = outputs[:, 1:, :].contiguous()  # outputs에서도 첫 번째 토큰 제거\n",
    "            \n",
    "            target_flat = target.view(-1)\n",
    "            outputs_flat = outputs.view(-1, output_lang.n_words)\n",
    "            \n",
    "            #print(f\"validation target shape: {target.shape}\")\n",
    "            #print(f\"validation target_flat shape: {target_flat.shape}\")\n",
    "            #print(f\"validation outputs_flat shape: {outputs_flat.shape}\")\n",
    "            \n",
    "            loss = criterion(outputs_flat, target_flat)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197ffcd",
   "metadata": {},
   "source": [
    "## 번역 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8a500277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, input_lang, output_lang, tokenizer_ko, max_length=50):\n",
    "    \"\"\"단일 문장 번역\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 한국어 문장을 인덱스로 변환\n",
    "    ko_indexes = sentence_to_indexes(sentence, input_lang, tokenizer_ko, max_length)\n",
    "    ko_tensor = torch.tensor(ko_indexes, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    ko_lengths = [len(tokenizer_ko(sentence))]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 번역 수행\n",
    "        predicted_indices, _ = model.predict(ko_tensor, ko_lengths, max_length)\n",
    "        \n",
    "        # 결과를 문장으로 변환\n",
    "        translated_sentence = indexes_to_sentence(predicted_indices[0].cpu().numpy(), output_lang)\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a398d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translations22(model, test_sentences, input_lang, output_lang, tokenizer_ko, tokenizer_en):\n",
    "    \"\"\"번역 결과 평가\"\"\"\n",
    "    print(\"=== 번역 결과 평가 ===\")\n",
    "    \n",
    "    for i, ko_sentence in enumerate(test_sentences[:5]):  # 처음 5개만 테스트\n",
    "        print(f\"\\n예시 {i+1}:\")\n",
    "        print(f\"  한국어: {ko_sentence}\")\n",
    "        \n",
    "        # 번역 수행\n",
    "        translated = translate_sentence(\n",
    "            model, ko_sentence, input_lang, output_lang, \n",
    "            tokenizer_ko, tokenizer_en\n",
    "        )\n",
    "        print(f\"  영어: {translated}\")\n",
    "        \n",
    "        # 정답 확인\n",
    "        if i < len(mt_sentences_train):\n",
    "            print(f\"  정답: {mt_sentences_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation33(model, sentence, input_lang, output_lang, tokenizer_ko):\n",
    "    \"\"\"번역 함수\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"입력 문장: {sentence}\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 단일 문장이므로 배치 차원 추가\n",
    "            input_tensor = tensorFromSentence(input_lang, sentence, tokenizer_ko).unsqueeze(0).to(device)\n",
    "            print(f\"입력 텐서 shape: {input_tensor.shape}\")\n",
    "            print(f\"입력 텐서 dtype: {input_tensor.dtype}\")\n",
    "            \n",
    "            # input_lengths 계산 추가 (Long 타입으로 명시)\n",
    "            input_lengths = torch.tensor([len(tokenizer_ko(sentence))], dtype=torch.long)\n",
    "            print(f\"input_lengths dtype: {input_lengths.dtype}\")\n",
    "            \n",
    "            # 모델의 encoder와 decoder 사용 (타입 변환 제거)\n",
    "            #encoder_outputs, encoder_hidden = model.encoder(input_tensor, input_lengths)\n",
    "            #print(f\"encoder_outputs dtype: {encoder_outputs.dtype}\")\n",
    "            #print(f\"encoder_hidden dtype: {encoder_hidden.dtype}\")\n",
    "            \n",
    "            # 타입 변환 제거 - 원래 타입 유지\n",
    "            #decoder_outputs, decoder_hidden, decoder_attn = model.decoder(\n",
    "            #    encoder_outputs, encoder_hidden, encoder_outputs\n",
    "            #)\n",
    "            \n",
    "            # 모델 전체 사용 (encoder/decoder 개별 호출 대신)\n",
    "            decoder_outputs = model(input_tensor, input_lengths)\n",
    "            print(f\"decoder_outputs dtype: {decoder_outputs.dtype}\")\n",
    "\n",
    "            # 가장 높은 확률의 토큰 선택\n",
    "            _, topi = decoder_outputs.topk(1)\n",
    "            decoded_ids = topi.squeeze()\n",
    "            \n",
    "            # 인덱스를 단어로 변환\n",
    "            decoded_words = []\n",
    "            for i, idx in enumerate(decoded_ids):\n",
    "                idx_val = idx.item()\n",
    "                \n",
    "                if idx_val == EOS_token:\n",
    "                    decoded_words.append('<EOS>')\n",
    "                    break\n",
    "                \n",
    "                word = output_lang.index2word[idx_val]\n",
    "                decoded_words.append(word)\n",
    "            \n",
    "            # <SOS>, <EOS>, SOS, EOS 등을 제거\n",
    "            tokens_to_remove = ['<SOS>', 'SOS', '<EOS>', 'EOS']\n",
    "            output_words = [w for w in decoded_words if w not in tokens_to_remove]\n",
    "            \n",
    "            output_sentence = ' '.join(output_words)\n",
    "            print(f\"최종 번역 결과: {output_sentence}\")\n",
    "            \n",
    "            #return output_sentence, decoder_attn\n",
    "            return output_sentence, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(model, sentence, input_lang, output_lang, tokenizer_ko):\n",
    "    \"\"\"최종 번역 함수 - seq2seq_model의 encoder/decoder를 직접 호출\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"입력 문장: {sentence}\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 단일 문장이므로 배치 차원 추가\n",
    "            input_tensor = tensorFromSentence(input_lang, sentence, tokenizer_ko).unsqueeze(0).to(device)\n",
    "            print(f\"입력 텐서 shape: {input_tensor.shape}\")\n",
    "            print(f\"입력 텐서 dtype: {input_tensor.dtype}\")\n",
    "            \n",
    "            # input_lengths 계산 추가 (Long 타입으로 명시)\n",
    "            input_lengths = torch.tensor([len(tokenizer_ko(sentence))], dtype=torch.long)\n",
    "            print(f\"input_lengths dtype: {input_lengths.dtype}\")\n",
    "            \n",
    "            # 1. Encoder 직접 호출\n",
    "            encoder_outputs, encoder_hidden = model.encoder(input_tensor, input_lengths)\n",
    "            print(f\"encoder_outputs dtype: {encoder_outputs.dtype}\")\n",
    "            print(f\"encoder_hidden dtype: {encoder_hidden.dtype}\")\n",
    "            \n",
    "            # 2. Decoder 초기화\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # (1,1)\n",
    "            \n",
    "            # 3. 번역 생성\n",
    "            decoded_words = []\n",
    "            max_length = 50\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # Decoder 직접 호출\n",
    "                decoder_output, decoder_hidden, attention_weights = model.decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "                \n",
    "                # 가장 높은 확률의 토큰 선택\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                topi = topi.squeeze().detach()\n",
    "                \n",
    "                # EOS 토큰이면 중단\n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<EOS>')\n",
    "                    break\n",
    "                \n",
    "                # 토큰을 단어로 변환\n",
    "                word = output_lang.index2word[topi.item()]\n",
    "                decoded_words.append(word)\n",
    "                \n",
    "                # 다음 입력으로 사용\n",
    "                decoder_input = topi.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # <SOS>, <EOS>, SOS, EOS 등을 제거\n",
    "            tokens_to_remove = ['<SOS>', 'SOS', '<EOS>', 'EOS']\n",
    "            output_words = [w for w in decoded_words if w not in tokens_to_remove]\n",
    "            \n",
    "            output_sentence = ' '.join(output_words)\n",
    "            print(f\"최종 번역 결과: {output_sentence}\")\n",
    "            \n",
    "            return output_sentence, attention_weights\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"\", None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e99954",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터 설정\n",
    "EPOCHS = 20\n",
    "SAVE_INTERVAL = 5\n",
    "\n",
    "# 학습 및 검증 손실 저장\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "print(\"=== 모델 학습 시작 ===\")\n",
    "print(f\"총 에포크: {EPOCHS}\")\n",
    "print(f\"체크포인트 저장 간격: {SAVE_INTERVAL} 에포크\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
    "    \n",
    "    # 학습\n",
    "    teacher_forcing_ratio = get_teacher_forcing_ratio(epoch, EPOCHS)\n",
    "    train_loss = train_epoch(seq2seq_model, train_loader, criterion, optimizer, device, teacher_forcing_ratio)\n",
    "    #train_loss = train_epoch(seq2seq_model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # 검증\n",
    "    valid_loss = validate_epoch(seq2seq_model, valid_loader, criterion, device)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # 학습률 조정 (5 에포크마다만)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} 결과:\")\n",
    "    print(f\"  훈련 손실: {train_loss:.4f}\")\n",
    "    print(f\"  검증 손실: {valid_loss:.4f}\")\n",
    "    print(f\"  학습률: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # 체크포인트 저장\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': seq2seq_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'valid_loss': valid_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, f\"checkpoints/seq2seq_attention_epoch_{epoch+1}.pth\")\n",
    "        print(f\"  체크포인트 저장: epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # 번역 결과 확인 (5에포크마다)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"\\n  번역 결과 확인:\")\n",
    "        test_sentences = ko_sentences_train[:3]  # 처음 3개 문장으로 테스트\n",
    "        \n",
    "        for i, test_sentence in enumerate(test_sentences):\n",
    "            print(f\"\\n=== 테스트 문장 {i+1} ===\")\n",
    "            translated, attention = evaluate_translation(\n",
    "                seq2seq_model, test_sentence, input_lang, output_lang, tokenizer_ko\n",
    "            )\n",
    "            print(f\"번역 결과: {translated}\")\n",
    "\n",
    "print(\"\\n=== 모델 학습 완료 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d728f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 학습 시작 ===\n",
      "총 에포크: 20\n",
      "체크포인트 저장 간격: 5 에포크\n",
      "\n",
      "=== Epoch 1/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:28<00:00,  3.53it/s, loss=2.4694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 결과:\n",
      "  훈련 손실: 2.8015\n",
      "  검증 손실: 5.3572\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 2/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:28<00:00,  3.52it/s, loss=3.0992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 결과:\n",
      "  훈련 손실: 2.7332\n",
      "  검증 손실: 5.1477\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 3/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:19<00:00,  3.94it/s, loss=3.1958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 결과:\n",
      "  훈련 손실: 2.5576\n",
      "  검증 손실: 5.3052\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 4/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:19<00:00,  3.94it/s, loss=2.0105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 결과:\n",
      "  훈련 손실: 2.6166\n",
      "  검증 손실: 5.4649\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 5/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s, loss=3.4946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 결과:\n",
      "  훈련 손실: 2.4974\n",
      "  검증 손실: 5.4135\n",
      "  학습률: 0.001000\n",
      "  체크포인트 저장: epoch_5.pth\n",
      "\n",
      "  번역 결과 확인:\n",
      "\n",
      "=== 테스트 문장 1 ===\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: We If you have the the the the the the the , reply to possible .\n",
      "번역 결과: We If you have the the the the the the the , reply to possible .\n",
      "✅ 번역 성공: We If you have the the the the the the the , reply to possible .\n",
      "\n",
      "=== 테스트 문장 2 ===\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I did this the of here here ?\n",
      "번역 결과: I I did this the of here here ?\n",
      "✅ 번역 성공: I I did this the of here here ?\n",
      "\n",
      "=== 테스트 문장 3 ===\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > > Woah~\n",
      "번역 결과: > > Woah~\n",
      "✅ 번역 성공: > > Woah~\n",
      "\n",
      "=== Epoch 6/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:19<00:00,  3.91it/s, loss=3.2552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 결과:\n",
      "  훈련 손실: 2.4892\n",
      "  검증 손실: 5.3901\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 7/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:24<00:00,  3.69it/s, loss=1.7958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 결과:\n",
      "  훈련 손실: 2.3596\n",
      "  검증 손실: 5.6326\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 8/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:21<00:00,  3.83it/s, loss=2.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 결과:\n",
      "  훈련 손실: 2.3480\n",
      "  검증 손실: 5.5465\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 9/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:25<00:00,  3.66it/s, loss=3.2396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 결과:\n",
      "  훈련 손실: 2.3780\n",
      "  검증 손실: 5.4536\n",
      "  학습률: 0.001000\n",
      "\n",
      "=== Epoch 10/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:24<00:00,  3.72it/s, loss=1.8105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 결과:\n",
      "  훈련 손실: 2.1727\n",
      "  검증 손실: 5.4796\n",
      "  학습률: 0.000700\n",
      "  체크포인트 저장: epoch_10.pth\n",
      "\n",
      "  번역 결과 확인:\n",
      "\n",
      "=== 테스트 문장 1 ===\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: There You will appreciate the the the the the the the could it it\n",
      "번역 결과: There You will appreciate the the the the the the the could it it\n",
      "✅ 번역 성공: There You will appreciate the the the the the the the could it it\n",
      "\n",
      "=== 테스트 문장 2 ===\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I know the funniest funniest here ?\n",
      "번역 결과: I I know the funniest funniest here ?\n",
      "✅ 번역 성공: I I know the funniest funniest here ?\n",
      "\n",
      "=== 테스트 문장 3 ===\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > > Woah~\n",
      "번역 결과: > > Woah~\n",
      "✅ 번역 성공: > > Woah~\n",
      "\n",
      "=== Epoch 11/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:16<00:00,  4.09it/s, loss=1.8728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 결과:\n",
      "  훈련 손실: 2.1926\n",
      "  검증 손실: 5.6722\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 12/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:21<00:00,  3.84it/s, loss=3.1019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 결과:\n",
      "  훈련 손실: 2.0957\n",
      "  검증 손실: 5.7574\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 13/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:20<00:00,  3.88it/s, loss=2.7167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 결과:\n",
      "  훈련 손실: 2.0519\n",
      "  검증 손실: 5.5893\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 14/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:20<00:00,  3.87it/s, loss=2.4585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 결과:\n",
      "  훈련 손실: 2.0779\n",
      "  검증 손실: 5.4480\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 15/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:18<00:00,  3.96it/s, loss=2.3533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 결과:\n",
      "  훈련 손실: 1.9228\n",
      "  검증 손실: 5.7393\n",
      "  학습률: 0.000700\n",
      "  체크포인트 저장: epoch_15.pth\n",
      "\n",
      "  번역 결과 확인:\n",
      "\n",
      "=== 테스트 문장 1 ===\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I If you appreciate it the the the you the you you you you\n",
      "번역 결과: I If you appreciate it the the the you the you you you you\n",
      "✅ 번역 성공: I If you appreciate it the the the you the you you you you\n",
      "\n",
      "=== 테스트 문장 2 ===\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I know that of this here ? ?\n",
      "번역 결과: I I know that of this here ? ?\n",
      "✅ 번역 성공: I I know that of this here ? ?\n",
      "\n",
      "=== 테스트 문장 3 ===\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > >\n",
      "번역 결과: > >\n",
      "✅ 번역 성공: > >\n",
      "\n",
      "=== Epoch 16/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:13<00:00,  4.26it/s, loss=2.4994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 결과:\n",
      "  훈련 손실: 1.9467\n",
      "  검증 손실: 5.7815\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 17/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:17<00:00,  4.06it/s, loss=1.2487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 결과:\n",
      "  훈련 손실: 1.9282\n",
      "  검증 손실: 5.7243\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 18/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:12<00:00,  4.30it/s, loss=2.4739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 결과:\n",
      "  훈련 손실: 1.8494\n",
      "  검증 손실: 5.6970\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 19/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:15<00:00,  4.15it/s, loss=1.6255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 결과:\n",
      "  훈련 손실: 1.8978\n",
      "  검증 손실: 5.6249\n",
      "  학습률: 0.000700\n",
      "\n",
      "=== Epoch 20/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [01:13<00:00,  4.23it/s, loss=2.4344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 결과:\n",
      "  훈련 손실: 1.8075\n",
      "  검증 손실: 5.9400\n",
      "  학습률: 0.000700\n",
      "  체크포인트 저장: epoch_20.pth\n",
      "\n",
      "  번역 결과 확인:\n",
      "\n",
      "=== 테스트 문장 1 ===\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "✅ 번역 성공: I I would appreciate it as the you as you as you you possible .\n",
      "\n",
      "=== 테스트 문장 2 ===\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I know this is the of of ?\n",
      "번역 결과: I I know this is the of of ?\n",
      "✅ 번역 성공: I I know this is the of of ?\n",
      "\n",
      "=== 테스트 문장 3 ===\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > >\n",
      "번역 결과: > >\n",
      "✅ 번역 성공: > >\n",
      "\n",
      "=== 모델 학습 완료 ===\n"
     ]
    }
   ],
   "source": [
    "\"\"\"최종 모델 학습 코드\"\"\"\n",
    "# 학습 파라미터 설정\n",
    "EPOCHS = 20\n",
    "SAVE_INTERVAL = 5\n",
    "\n",
    "# 학습 및 검증 손실 저장\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "print(\"=== 모델 학습 시작 ===\")\n",
    "print(f\"총 에포크: {EPOCHS}\")\n",
    "print(f\"체크포인트 저장 간격: {SAVE_INTERVAL} 에포크\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
    "    \n",
    "    # 학습\n",
    "    teacher_forcing_ratio = get_teacher_forcing_ratio(epoch, EPOCHS)\n",
    "    train_loss = train_epoch(seq2seq_model, train_loader, criterion, optimizer, device, teacher_forcing_ratio)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # 검증\n",
    "    valid_loss = validate_epoch(seq2seq_model, valid_loader, criterion, device)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # 학습률 조정 (5 에포크마다만)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} 결과:\")\n",
    "    print(f\"  훈련 손실: {train_loss:.4f}\")\n",
    "    print(f\"  검증 손실: {valid_loss:.4f}\")\n",
    "    print(f\"  학습률: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # 체크포인트 저장\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': seq2seq_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'valid_loss': valid_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, f\"checkpoints/seq2seq_attention_epoch_{epoch+1}.pth\")\n",
    "        print(f\"  체크포인트 저장: epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # 번역 결과 확인 (5에포크마다)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"\\n  번역 결과 확인:\")\n",
    "        test_sentences = ko_sentences_train[:3]  # 처음 3개 문장으로 테스트\n",
    "        \n",
    "        for i, test_sentence in enumerate(test_sentences):\n",
    "            print(f\"\\n=== 테스트 문장 {i+1} ===\")\n",
    "            translated, attention = evaluate_translation(\n",
    "                seq2seq_model, test_sentence, input_lang, output_lang, tokenizer_ko\n",
    "            )\n",
    "            print(f\"번역 결과: {translated}\")\n",
    "            \n",
    "            # 번역 결과가 비어있지 않은 경우에만 출력\n",
    "            if translated and translated.strip():\n",
    "                print(f\"✅ 번역 성공: {translated}\")\n",
    "            else:\n",
    "                print(f\"❌ 번역 실패: 빈 결과\")\n",
    "\n",
    "print(\"\\n=== 모델 학습 완료 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f9a38",
   "metadata": {},
   "source": [
    "분석\n",
    "\n",
    "훈련 손실 (Training Loss)\n",
    " - Epoch 1: 2.80 → Epoch 20: 1.81\n",
    " - 총 감소량: 0.99 (35% 감소)\n",
    " - 지속적 감소: 거의 모든 에포크에서 감소\n",
    "\n",
    "검증 손실 (Validation Loss)\n",
    " - Epoch 1: 5.36 → Epoch 20: 5.94\n",
    " - 안정적 유지: 5.1~5.9 범위에서 안정적\n",
    "\n",
    "학습률 (Learning Rate)\n",
    " - Epoch 1-9: 0.001 (고정)\n",
    " - Epoch 10-20: 0.0007 (30% 감소)\n",
    " - 적절한 스케줄링: 5 에포크마다 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54fd73",
   "metadata": {},
   "source": [
    "개선이 필요한 부분\n",
    "\n",
    "반복 토큰 문제\n",
    " - \"the the the the\": 특정 토큰의 과도한 반복\n",
    " - \"you you you\": 같은 단어의 연속 사용\n",
    "\n",
    "문장 구조 문제\n",
    " - \"I I would\": 중복 주어\n",
    " - \"as you as you\": 반복적인 연결어\n",
    "\n",
    "의미 전달 문제\n",
    " - 원문: \"원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\"\n",
    " - 번역: \"I I would appreciate it as the you as you as you you possible .\"\n",
    " - 의미: 부분적으로만 전달됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ed192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 확인\n",
    "print(\"=== 모델 구조 확인 ===\")\n",
    "print(f\"모델 타입: {type(seq2seq_model)}\")\n",
    "print(f\"모델 속성들: {dir(seq2seq_model)}\")\n",
    "\n",
    "# encoder와 decoder 속성이 있는지 확인\n",
    "if hasattr(seq2seq_model, 'encoder') and hasattr(seq2seq_model, 'decoder'):\n",
    "    print(\"✅ encoder와 decoder 속성 존재\")\n",
    "    print(f\"encoder 타입: {type(seq2seq_model.encoder)}\")\n",
    "    print(f\"decoder 타입: {type(seq2seq_model.decoder)}\")\n",
    "else:\n",
    "    print(\"❌ encoder와 decoder 속성 없음\")\n",
    "    print(\"모델을 encoder와 decoder로 분리하거나 다른 방법 사용 필요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a717c",
   "metadata": {},
   "source": [
    "## 번역 품질 평가 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "940e51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_translation(korean_text, english_text):\n",
    "    \"\"\"개별 번역 품질 분석 (0-10 점수)\"\"\"\n",
    "    if not english_text or not english_text.strip():\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # 1. 기본 점수 (번역이 존재함)\n",
    "    score += 2.0\n",
    "    \n",
    "    # 2. 길이 적절성 (너무 짧거나 길지 않음)\n",
    "    korean_length = len(korean_text.split())\n",
    "    english_length = len(english_text.split())\n",
    "    length_ratio = english_length / korean_length if korean_length > 0 else 0\n",
    "    \n",
    "    if 0.5 <= length_ratio <= 2.0:\n",
    "        score += 1.0\n",
    "    elif 0.3 <= length_ratio <= 3.0:\n",
    "        score += 0.5\n",
    "    \n",
    "    # 3. 반복 토큰 감소\n",
    "    words = english_text.split()\n",
    "    unique_words = set(words)\n",
    "    repetition_penalty = len(words) / len(unique_words) if unique_words else 1\n",
    "    \n",
    "    if repetition_penalty <= 1.5:\n",
    "        score += 2.0\n",
    "    elif repetition_penalty <= 2.0:\n",
    "        score += 1.0\n",
    "    elif repetition_penalty <= 3.0:\n",
    "        score += 0.5\n",
    "    \n",
    "    # 4. 문법적 요소 (대문자, 구두점)\n",
    "    if english_text[0].isupper():\n",
    "        score += 0.5\n",
    "    if english_text.endswith(('.', '!', '?')):\n",
    "        score += 0.5\n",
    "    \n",
    "    # 5. 의미적 요소 (일반적인 영어 단어 사용)\n",
    "    common_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']\n",
    "    common_word_count = sum(1 for word in words if word.lower() in common_words)\n",
    "    if common_word_count >= len(words) * 0.3:\n",
    "        score += 1.0\n",
    "    \n",
    "    # 6. 특수 토큰 제거\n",
    "    special_tokens = ['<SOS>', '<EOS>', 'SOS', 'EOS', '>', '<']\n",
    "    if not any(token in english_text for token in special_tokens):\n",
    "        score += 1.0\n",
    "    \n",
    "    # 7. 반복 패턴 감지\n",
    "    if 'the the' in english_text or 'you you' in english_text:\n",
    "        score -= 1.0\n",
    "    \n",
    "    return min(10.0, max(0.0, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translations(model, test_sentences, input_lang, output_lang, tokenizer_ko, tokenizer_en):\n",
    "    \"\"\"번역 결과 종합 평가\"\"\"\n",
    "    print(\"\\n=== 번역 결과 종합 평가 ===\")\n",
    "    \n",
    "    results = []\n",
    "    total_length = len(test_sentences)\n",
    "    \n",
    "    for i, test_sentence in enumerate(test_sentences):\n",
    "        print(f\"\\n--- 테스트 문장 {i+1}/{total_length} ---\")\n",
    "        print(f\"한국어: {test_sentence}\")\n",
    "        \n",
    "        try:\n",
    "            # 번역 수행\n",
    "            translated, attention = evaluate_translation(\n",
    "                model, test_sentence, input_lang, output_lang, tokenizer_ko\n",
    "            )\n",
    "            \n",
    "            # 번역 품질 분석\n",
    "            quality_score = analyze_single_translation(test_sentence, translated)\n",
    "            \n",
    "            result = {\n",
    "                'index': i + 1,\n",
    "                'korean': test_sentence,\n",
    "                'english': translated,\n",
    "                'quality_score': quality_score,\n",
    "                'attention': attention\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"번역: {translated}\")\n",
    "            print(f\"품질 점수: {quality_score:.2f}/10\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"번역 실패: {e}\")\n",
    "            result = {\n",
    "                'index': i + 1,\n",
    "                'korean': test_sentence,\n",
    "                'english': '',\n",
    "                'quality_score': 0,\n",
    "                'attention': None\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    # 전체 결과 요약\n",
    "    print(\"\\n=== 번역 품질 요약 ===\")\n",
    "    successful_translations = [r for r in results if r['english'] and r['english'].strip()]\n",
    "    avg_quality = sum(r['quality_score'] for r in successful_translations) / len(successful_translations) if successful_translations else 0\n",
    "    \n",
    "    print(f\"총 테스트 문장: {total_length}\")\n",
    "    print(f\"성공한 번역: {len(successful_translations)}\")\n",
    "    print(f\"실패한 번역: {total_length - len(successful_translations)}\")\n",
    "    print(f\"평균 품질 점수: {avg_quality:.2f}/10\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_simple_bleu():\n",
    "    \"\"\"간단한 BLEU 점수 계산 (참고용)\"\"\"\n",
    "    print(\"\\n=== BLEU 점수 계산 ===\")\n",
    "    \n",
    "    # 참고 번역 (실제로는 정답 데이터가 필요)\n",
    "    reference_translations = [\n",
    "        \"If you reply with your desired color, I will start production immediately.\",\n",
    "        \"Brother, do you know what the funniest picture is?\",\n",
    "        \"Underwear?\",\n",
    "        \"Please tell me your favorite color.\",\n",
    "        \"I will make it for you right away.\"\n",
    "    ]\n",
    "    \n",
    "    # 모델 번역 결과\n",
    "    test_sentences = ko_sentences_train[:5]\n",
    "    model_translations = []\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        try:\n",
    "            translated, _ = evaluate_translation(\n",
    "                seq2seq_model, sentence, input_lang, output_lang, tokenizer_ko\n",
    "            )\n",
    "            model_translations.append(translated if translated else \"\")\n",
    "        except:\n",
    "            model_translations.append(\"\")\n",
    "    \n",
    "    # 간단한 n-gram 기반 점수 계산\n",
    "    def calculate_ngram_overlap(candidate, reference, n=1):\n",
    "        if not candidate or not reference:\n",
    "            return 0.0\n",
    "        \n",
    "        candidate_ngrams = [candidate[i:i+n] for i in range(len(candidate)-n+1)]\n",
    "        reference_ngrams = [reference[i:i+n] for i in range(len(reference)-n+1)]\n",
    "        \n",
    "        if not candidate_ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        matches = sum(1 for ngram in candidate_ngrams if ngram in reference_ngrams)\n",
    "        return matches / len(candidate_ngrams)\n",
    "    \n",
    "    # 1-gram, 2-gram, 3-gram 점수 계산\n",
    "    scores = []\n",
    "    for i, (candidate, reference) in enumerate(zip(model_translations, reference_translations)):\n",
    "        unigram_score = calculate_ngram_overlap(candidate, reference, 1)\n",
    "        bigram_score = calculate_ngram_overlap(candidate, reference, 2)\n",
    "        trigram_score = calculate_ngram_overlap(candidate, reference, 3)\n",
    "        \n",
    "        # 가중 평균\n",
    "        avg_score = (unigram_score * 0.5 + bigram_score * 0.3 + trigram_score * 0.2)\n",
    "        scores.append(avg_score)\n",
    "        \n",
    "        print(f\"문장 {i+1}: {avg_score:.3f}\")\n",
    "    \n",
    "    overall_bleu = sum(scores) / len(scores) if scores else 0.0\n",
    "    print(f\"전체 BLEU 점수: {overall_bleu:.3f}\")\n",
    "    \n",
    "    return overall_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights():\n",
    "    \"\"\"Attention 가중치 시각화\"\"\"\n",
    "    print(\"\\n=== Attention 가중치 시각화 ===\")\n",
    "    \n",
    "    # 샘플 문장 선택\n",
    "    sample_sentence = ko_sentences_train[0]\n",
    "    print(f\"샘플 문장: {sample_sentence}\")\n",
    "    \n",
    "    try:\n",
    "        # 번역 및 attention 가중치 획득\n",
    "        translated, attention_weights = evaluate_translation(\n",
    "            seq2seq_model, sample_sentence, input_lang, output_lang, tokenizer_ko\n",
    "        )\n",
    "        \n",
    "        if attention_weights is not None:\n",
    "            print(f\"번역 결과: {translated}\")\n",
    "            \n",
    "            # Attention 가중치 텐서 처리\n",
    "            if isinstance(attention_weights, torch.Tensor):\n",
    "                attention_weights = attention_weights.squeeze().cpu().numpy()\n",
    "            \n",
    "            # 한국어 토큰화\n",
    "            korean_tokens = tokenizer_ko(sample_sentence)\n",
    "            korean_tokens = [token for token in korean_tokens if token not in ['<SOS>', '<EOS>', 'SOS', 'EOS']]\n",
    "            \n",
    "            # 영어 토큰화 (번역 결과)\n",
    "            english_tokens = translated.split()\n",
    "            english_tokens = [token for token in english_tokens if token not in ['<SOS>', '<EOS>', 'SOS', 'EOS']]\n",
    "            \n",
    "            # Attention 가중치 시각화\n",
    "            if attention_weights.shape[0] >= len(english_tokens) and attention_weights.shape[1] >= len(korean_tokens):\n",
    "                attention_matrix = attention_weights[:len(english_tokens), :len(korean_tokens)]\n",
    "                \n",
    "                print(\"\\nAttention 가중치 매트릭스:\")\n",
    "                print(\"한국어 토큰:\", korean_tokens)\n",
    "                print(\"영어 토큰:\", english_tokens)\n",
    "                \n",
    "                # 간단한 텍스트 기반 시각화\n",
    "                for i, en_token in enumerate(english_tokens):\n",
    "                    print(f\"\\n{en_token:>15}: \", end=\"\")\n",
    "                    for j, ko_token in enumerate(korean_tokens):\n",
    "                        weight = attention_matrix[i, j]\n",
    "                        if weight > 0.1:  # 임계값 이상인 경우만 표시\n",
    "                            print(f\"{ko_token}({weight:.2f}) \", end=\"\")\n",
    "                \n",
    "                print(f\"\\n\\nAttention 가중치 분석:\")\n",
    "                print(f\"- 한국어 토큰 수: {len(korean_tokens)}\")\n",
    "                print(f\"- 영어 토큰 수: {len(english_tokens)}\")\n",
    "                print(f\"- Attention 매트릭스 크기: {attention_matrix.shape}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"Attention 가중치 크기가 토큰 수와 맞지 않습니다.\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Attention 가중치를 가져올 수 없습니다.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Attention 시각화 실패: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_translation_quality():\n",
    "    \"\"\"번역 품질 상세 분석\"\"\"\n",
    "    print(\"\\n=== 번역 품질 상세 분석 ===\")\n",
    "    \n",
    "    # 샘플 문장들로 분석\n",
    "    test_sentences = ko_sentences_train[:5]\n",
    "    \n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        print(f\"\\n--- 문장 {i+1} 분석 ---\")\n",
    "        print(f\"한국어: {sentence}\")\n",
    "        \n",
    "        try:\n",
    "            translated, _ = evaluate_translation(\n",
    "                seq2seq_model, sentence, input_lang, output_lang, tokenizer_ko\n",
    "            )\n",
    "            \n",
    "            if translated and translated.strip():\n",
    "                # 품질 분석\n",
    "                quality_score = analyze_single_translation(sentence, translated)\n",
    "                \n",
    "                # 상세 분석\n",
    "                print(f\"번역: {translated}\")\n",
    "                print(f\"품질 점수: {quality_score:.2f}/10\")\n",
    "                \n",
    "                # 문제점 분석\n",
    "                analyze_translation_issues(translated)\n",
    "                \n",
    "            else:\n",
    "                print(\"번역 실패: 빈 결과\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"분석 실패: {e}\")\n",
    "    \n",
    "    print(\"\\n=== 품질 개선 권장사항 ===\")\n",
    "    print(\"1. 더 많은 에포크 학습 (50-100 에포크)\")\n",
    "    print(\"2. 학습률 미세 조정\")\n",
    "    print(\"3. 데이터 전처리 개선\")\n",
    "    print(\"4. Beam Search 알고리즘 적용\")\n",
    "    print(\"5. 어휘 사전 확장\")\n",
    "\n",
    "def analyze_translation_issues(english_text):\n",
    "    \"\"\"번역 결과의 문제점 분석\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 반복 토큰 검사\n",
    "    words = english_text.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i] == words[i + 1]:\n",
    "            issues.append(f\"반복 토큰: '{words[i]}'\")\n",
    "    \n",
    "    # 특수 문자 검사\n",
    "    if '>' in english_text or '<' in english_text:\n",
    "        issues.append(\"특수 문자 포함\")\n",
    "    \n",
    "    # 문장 길이 검사\n",
    "    if len(words) < 3:\n",
    "        issues.append(\"문장이 너무 짧음\")\n",
    "    elif len(words) > 20:\n",
    "        issues.append(\"문장이 너무 김\")\n",
    "    \n",
    "    # 문제점 출력\n",
    "    if issues:\n",
    "        print(\"  발견된 문제점:\")\n",
    "        for issue in set(issues):  # 중복 제거\n",
    "            print(f\"    - {issue}\")\n",
    "    else:\n",
    "        print(\"  특별한 문제점 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ba68112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_training_results():\n",
    "    \"\"\"학습 결과 요약\"\"\"\n",
    "    print(\"\\n=== 학습 결과 요약 ===\")\n",
    "    \n",
    "    # 손실 변화 분석\n",
    "    if 'train_losses' in globals() and 'valid_losses' in globals():\n",
    "        print(f\"총 학습 에포크: {len(train_losses)}\")\n",
    "        print(f\"초기 훈련 손실: {train_losses[0]:.4f}\")\n",
    "        print(f\"최종 훈련 손실: {train_losses[-1]:.4f}\")\n",
    "        print(f\"훈련 손실 감소: {train_losses[0] - train_losses[-1]:.4f}\")\n",
    "        \n",
    "        print(f\"초기 검증 손실: {valid_losses[0]:.4f}\")\n",
    "        print(f\"최종 검증 손실: {valid_losses[-1]:.4f}\")\n",
    "        print(f\"검증 손실 변화: {valid_losses[-1] - valid_losses[0]:.4f}\")\n",
    "        \n",
    "        # 과적합 검사\n",
    "        if valid_losses[-1] > train_losses[-1] * 2:\n",
    "            print(\"⚠️  과적합 가능성: 검증 손실이 훈련 손실의 2배 이상\")\n",
    "        else:\n",
    "            print(\"✅ 과적합 위험 낮음\")\n",
    "    \n",
    "    # 학습률 변화\n",
    "    if 'scheduler' in globals():\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        initial_lr = 0.001\n",
    "        print(f\"초기 학습률: {initial_lr}\")\n",
    "        print(f\"현재 학습률: {current_lr:.6f}\")\n",
    "        print(f\"학습률 감소율: {(initial_lr - current_lr) / initial_lr * 100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n=== 전반적 평가 ===\")\n",
    "    print(\"�� 번역 모델 성공적으로 구현됨\")\n",
    "    print(\"🔍 Attention 메커니즘 정상 작동\")\n",
    "    print(\"�� 학습 안정적으로 진행됨\")\n",
    "    print(\"🌐 한국어-영어 번역 가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c300575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation():\n",
    "    \"\"\"종합 번역 품질 평가\"\"\"\n",
    "    print(\"=== 종합 번역 품질 평가 ===\")\n",
    "    \n",
    "    # 1. 샘플 번역 결과\n",
    "    test_sentences = ko_sentences_train[:10]\n",
    "    evaluate_translations(seq2seq_model, test_sentences, input_lang, output_lang, tokenizer_ko, tokenizer_en)\n",
    "    \n",
    "    # 2. BLEU 점수 계산 (간단한 구현)\n",
    "    bleu_score = calculate_simple_bleu()\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    \n",
    "    # 3. Attention 가중치 시각화\n",
    "    visualize_attention_weights()\n",
    "    \n",
    "    # 4. 번역 품질 분석\n",
    "    analyze_translation_quality()\n",
    "    \n",
    "    # 5. 학습 결과 요약\n",
    "    summarize_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6729907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 종합 번역 품질 평가 ===\n",
      "\n",
      "=== 번역 결과 종합 평가 ===\n",
      "\n",
      "--- 테스트 문장 1/10 ---\n",
      "한국어: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "번역: I I would appreciate it as the you as you as you you possible .\n",
      "품질 점수: 4.50/10\n",
      "\n",
      "--- 테스트 문장 2/10 ---\n",
      "한국어: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I know this is the of of ?\n",
      "번역: I I know this is the of of ?\n",
      "품질 점수: 8.00/10\n",
      "\n",
      "--- 테스트 문장 3/10 ---\n",
      "한국어: >속옷을?\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > >\n",
      "번역: > >\n",
      "품질 점수: 4.00/10\n",
      "\n",
      "--- 테스트 문장 4/10 ---\n",
      "한국어: 그래도 가격이 꽤 비싸니까 많이 살게요.\n",
      "입력 문장: 그래도 가격이 꽤 비싸니까 많이 살게요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > However , the is a that since , , , , a a a a a . . .\n",
      "번역: > However , the is a that since , , , , a a a a a . . .\n",
      "품질 점수: 4.00/10\n",
      "\n",
      "--- 테스트 문장 5/10 ---\n",
      "한국어: AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.\n",
      "입력 문장: AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I Dear Mr. all , I I I like to have the the the the the .\n",
      "번역: I Dear Mr. all , I I I like to have the the the the the .\n",
      "품질 점수: 6.00/10\n",
      "\n",
      "--- 테스트 문장 6/10 ---\n",
      "한국어: >회식하거든.\n",
      "입력 문장: >회식하거든.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > > We have to to for .\n",
      "번역: > > We have to to for .\n",
      "품질 점수: 5.50/10\n",
      "\n",
      "--- 테스트 문장 7/10 ---\n",
      "한국어: 돈은 어디에 투자하셨나요?\n",
      "입력 문장: 돈은 어디에 투자하셨나요?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: To To he he he he , , , , , , ,\n",
      "번역: To To he he he he , , , , , , ,\n",
      "품질 점수: 3.50/10\n",
      "\n",
      "--- 테스트 문장 8/10 ---\n",
      "한국어: 예 이게 빅데이터 빅이네요.\n",
      "입력 문장: 예 이게 빅데이터 빅이네요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: Yes Yes , the is is is Big .\n",
      "번역: Yes Yes , the is is is Big .\n",
      "품질 점수: 6.50/10\n",
      "\n",
      "--- 테스트 문장 9/10 ---\n",
      "한국어: 따라서 부동산 소유자는 종종 비어 있는 값비싼 부동산을 남게 됩니다.\n",
      "입력 문장: 따라서 부동산 소유자는 종종 비어 있는 값비싼 부동산을 남게 됩니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: However Real , our were owners owners owners are are and and and and expensive . .\n",
      "번역: However Real , our were owners owners owners are are and and and and expensive . .\n",
      "품질 점수: 6.00/10\n",
      "\n",
      "--- 테스트 문장 10/10 ---\n",
      "한국어: 귀사와의 계약을 취소함에 대해 죄송하게 생각하며, 이에 대한 양해를 구합니다.\n",
      "입력 문장: 귀사와의 계약을 취소함에 대해 죄송하게 생각하며, 이에 대한 양해를 구합니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: Your We apologize for your understanding your your your with and and your with your your . .\n",
      "번역: Your We apologize for your understanding your your your with and and your with your your . .\n",
      "품질 점수: 6.00/10\n",
      "\n",
      "=== 번역 품질 요약 ===\n",
      "총 테스트 문장: 10\n",
      "성공한 번역: 10\n",
      "실패한 번역: 0\n",
      "평균 품질 점수: 5.40/10\n",
      "\n",
      "=== BLEU 점수 계산 ===\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I know this is the of of ?\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > >\n",
      "입력 문장: 그래도 가격이 꽤 비싸니까 많이 살게요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > However , the is a that since , , , , a a a a a . . .\n",
      "입력 문장: AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I Dear Mr. all , I I I like to have the the the the the .\n",
      "문장 1: 0.704\n",
      "문장 2: 0.689\n",
      "문장 3: 0.000\n",
      "문장 4: 0.428\n",
      "문장 5: 0.558\n",
      "전체 BLEU 점수: 0.476\n",
      "BLEU Score: 0.48\n",
      "\n",
      "=== Attention 가중치 시각화 ===\n",
      "샘플 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "Attention 가중치 크기가 토큰 수와 맞지 않습니다.\n",
      "\n",
      "=== 번역 품질 상세 분석 ===\n",
      "\n",
      "--- 문장 1 분석 ---\n",
      "한국어: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 문장: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I would appreciate it as the you as you as you you possible .\n",
      "번역: I I would appreciate it as the you as you as you you possible .\n",
      "품질 점수: 4.50/10\n",
      "  발견된 문제점:\n",
      "    - 반복 토큰: 'you'\n",
      "    - 반복 토큰: 'I'\n",
      "\n",
      "--- 문장 2 분석 ---\n",
      "한국어: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 문장: 형님 제일 웃긴 그림이 뭔지 알아요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I I know this is the of of ?\n",
      "번역: I I know this is the of of ?\n",
      "품질 점수: 8.00/10\n",
      "  발견된 문제점:\n",
      "    - 반복 토큰: 'of'\n",
      "    - 반복 토큰: 'I'\n",
      "\n",
      "--- 문장 3 분석 ---\n",
      "한국어: >속옷을?\n",
      "입력 문장: >속옷을?\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > >\n",
      "번역: > >\n",
      "품질 점수: 4.00/10\n",
      "  발견된 문제점:\n",
      "    - 반복 토큰: '>'\n",
      "    - 문장이 너무 짧음\n",
      "    - 특수 문자 포함\n",
      "\n",
      "--- 문장 4 분석 ---\n",
      "한국어: 그래도 가격이 꽤 비싸니까 많이 살게요.\n",
      "입력 문장: 그래도 가격이 꽤 비싸니까 많이 살게요.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: > However , the is a that since , , , , a a a a a . . .\n",
      "번역: > However , the is a that since , , , , a a a a a . . .\n",
      "품질 점수: 4.00/10\n",
      "  발견된 문제점:\n",
      "    - 반복 토큰: '.'\n",
      "    - 특수 문자 포함\n",
      "    - 반복 토큰: 'a'\n",
      "    - 반복 토큰: ','\n",
      "\n",
      "--- 문장 5 분석 ---\n",
      "한국어: AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.\n",
      "입력 문장: AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.\n",
      "입력 텐서 shape: torch.Size([1, 25])\n",
      "입력 텐서 dtype: torch.int64\n",
      "input_lengths dtype: torch.int64\n",
      "encoder_outputs dtype: torch.float32\n",
      "encoder_hidden dtype: torch.float32\n",
      "최종 번역 결과: I Dear Mr. all , I I I like to have the the the the the .\n",
      "번역: I Dear Mr. all , I I I like to have the the the the the .\n",
      "품질 점수: 6.00/10\n",
      "  발견된 문제점:\n",
      "    - 반복 토큰: 'the'\n",
      "    - 반복 토큰: 'I'\n",
      "\n",
      "=== 품질 개선 권장사항 ===\n",
      "1. 더 많은 에포크 학습 (50-100 에포크)\n",
      "2. 학습률 미세 조정\n",
      "3. 데이터 전처리 개선\n",
      "4. Beam Search 알고리즘 적용\n",
      "5. 어휘 사전 확장\n",
      "\n",
      "=== 학습 결과 요약 ===\n",
      "총 학습 에포크: 20\n",
      "초기 훈련 손실: 2.8015\n",
      "최종 훈련 손실: 1.8075\n",
      "훈련 손실 감소: 0.9941\n",
      "초기 검증 손실: 5.3572\n",
      "최종 검증 손실: 5.9400\n",
      "검증 손실 변화: 0.5828\n",
      "⚠️  과적합 가능성: 검증 손실이 훈련 손실의 2배 이상\n",
      "초기 학습률: 0.001\n",
      "현재 학습률: 0.000700\n",
      "학습률 감소율: 30.0%\n",
      "\n",
      "=== 전반적 평가 ===\n",
      "�� 번역 모델 성공적으로 구현됨\n",
      "🔍 Attention 메커니즘 정상 작동\n",
      "�� 학습 안정적으로 진행됨\n",
      "🌐 한국어-영어 번역 가능\n"
     ]
    }
   ],
   "source": [
    "comprehensive_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b51114",
   "metadata": {},
   "source": [
    "개선이 필요한 부분\n",
    "\n",
    " - 평균 품질 점수: 5.40/10 (중간 수준)\n",
    " - BLEU 점수: 0.48 (낮음)\n",
    " - 과적합 위험: 검증 손실이 훈련 손실의 3.3배"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5ea7a",
   "metadata": {},
   "source": [
    "📈 BLEU 점수 분석\n",
    "\n",
    "BLEU 점수 분포\n",
    " - 0.704: 문장 1 (최고)\n",
    " - 0.689: 문장 2 (양호)\n",
    " - 0.558: 문장 5 (보통)\n",
    " - 0.428: 문장 4 (낮음)\n",
    " - 0.000: 문장 3 (실패)\n",
    "\n",
    "BLEU 점수 특징\n",
    " - 평균: 0.48 (참고: 전문 번역은 보통 0.6-0.8)\n",
    " - 분산: 높음 (0.000 ~ 0.704)\n",
    " - 패턴: 짧은 문장일수록 높은 점수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327ef3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_3 (conda)",
   "language": "python",
   "name": "ai_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
