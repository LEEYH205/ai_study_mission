{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e52a21",
   "metadata": {},
   "source": [
    "\n",
    "# 🇰🇷→🇺🇸 NMT Slim Version — Seq2Seq (GRU) + (Optional) Bahdanau Attention\n",
    "\n",
    "**목표(Goal)**  \n",
    "- 최소 구성의 **Seq2Seq(GRU)**와 **어텐션(Bahdanau)** 버전을 빠르게 학습/평가할 수 있는 **슬림 노트북**입니다.  \n",
    "- **SentencePiece**가 없을 경우 자동으로 **공백 기반 토크나이저**로 폴백합니다.  \n",
    "- 학습 파일이 없으면 **토이 데이터**를 자동 생성해 end-to-end 테스트가 가능합니다.\n",
    "\n",
    "**핵심 포인트**  \n",
    "- `collate_fn`에서 **BOS/EOS 포함 길이 clamp**  \n",
    "- `pack_padded_sequence(enforce_sorted=False)` + `pad_packed_sequence(total_length=...)`  \n",
    "- **Loss 타깃 시프트**(`logits[:, :-1]` vs `tgt[:, 1:]`)  \n",
    "- 간단 **BLEU**(sacrebleu 있으면 사용, 없으면 내부 간이 구현)\n",
    "\n",
    "> 실행 순서: 위에서 아래로 순서대로 실행하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0c8929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_json': '일상생활및구어체_한영/일상생활및구어체_한영_train_set.json',\n",
       " 'valid_json': '일상생활및구어체_한영/일상생활및구어체_한영_valid_set.json',\n",
       " 'use_sentencepiece': True,\n",
       " 'spm_vocab_ko': 8000,\n",
       " 'spm_vocab_en': 8000,\n",
       " 'src_max_len': 64,\n",
       " 'tgt_max_len': 64,\n",
       " 'batch_size': 128,\n",
       " 'emb': 256,\n",
       " 'enc_hid': 256,\n",
       " 'dec_hid': 256,\n",
       " 'epochs': 3,\n",
       " 'lr': 0.002,\n",
       " 'teacher_forcing_start': 1.0,\n",
       " 'teacher_forcing_end': 0.5,\n",
       " 'use_attention': True,\n",
       " 'use_scheduler': False,\n",
       " 'use_checkpoint': False,\n",
       " 'device': 'mps'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =====================\n",
    "# Config\n",
    "# =====================\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    # 데이터 경로 (존재하지 않으면 토이 데이터 자동 생성)\n",
    "    \"train_json\": \"일상생활및구어체_한영/일상생활및구어체_한영_train_set.json\",\n",
    "    \"valid_json\": \"일상생활및구어체_한영/일상생활및구어체_한영_valid_set.json\",\n",
    "\n",
    "    # 토크나이저\n",
    "    \"use_sentencepiece\": True,      # sentencepiece 미설치 시 자동 폴백\n",
    "    \"spm_vocab_ko\": 8000,\n",
    "    \"spm_vocab_en\": 8000,\n",
    "    \n",
    "    # 길이/배치\n",
    "    \"src_max_len\": 64,\n",
    "    \"tgt_max_len\": 64,\n",
    "    \"batch_size\": 128,\n",
    "    \n",
    "    # 모델 크기\n",
    "    \"emb\": 256,\n",
    "    \"enc_hid\": 256,\n",
    "    \"dec_hid\": 256,\n",
    "    \n",
    "    # 학습\n",
    "    \"epochs\": 3,                # 빠른 테스트를 위해 소수 에폭\n",
    "    \"lr\": 2e-3,\n",
    "    \"teacher_forcing_start\": 1.0,\n",
    "    \"teacher_forcing_end\": 0.5,\n",
    "    \n",
    "    # 선택\n",
    "    \"use_attention\": True,      # Bahdanau Attention 사용\n",
    "    \"use_scheduler\": False,     # 슬림 버전 기본 OFF\n",
    "    \"use_checkpoint\": False,    # 슬림 버전 기본 OFF\n",
    "    \"device\": \"mps\",           # \"cuda\" 또는 \"cpu\" (자동 감지 로직도 아래에서 처리)\n",
    "}\n",
    "\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eec0e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =====================\n",
    "# Imports & Seed\n",
    "# =====================\n",
    "import os, json, math, random, re, sys, time\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "    HAS_SPM = True\n",
    "except Exception:\n",
    "    HAS_SPM = False\n",
    "\n",
    "try:\n",
    "    import sacrebleu\n",
    "    HAS_SACREBLEU = True\n",
    "except Exception:\n",
    "    HAS_SACREBLEU = False\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # 에러 없이 안전하게 처리\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    except:\n",
    "        pass  # CUDA가 없으면 무시\n",
    "    \n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# device\n",
    "CONFIG[\"device\"] = \"mps\" if (CONFIG[\"device\"]==\"mps\" and torch.backends.mps.is_available()) else \"cpu\"\n",
    "CONFIG[\"device\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49c5dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAS_SPM, HAS_SACREBLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ca949f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =====================\n",
    "# Data Utils\n",
    "# =====================\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def ensure_data(train_path, valid_path):\n",
    "    \"\"\"데이터 파일이 없으면 토이 데이터(작은 병렬 말뭉치)를 생성합니다.\"\"\"\n",
    "    tp, vp = Path(train_path), Path(valid_path)\n",
    "    if tp.exists() and vp.exists():\n",
    "        train, valid = load_json(train_path), load_json(valid_path)\n",
    "        return train, valid\n",
    "    \n",
    "    print(\"[INFO] 데이터 파일을 찾지 못했습니다. 토이 데이터를 생성합니다.\")\n",
    "    toy_pairs = [\n",
    "        {\"ko\": \"안녕하세요\", \"mt\": \"Hello\"},\n",
    "        {\"ko\": \"오늘 날씨 어때요?\", \"mt\": \"How is the weather today?\"},\n",
    "        {\"ko\": \"이름이 뭐예요?\", \"mt\": \"What is your name?\"},\n",
    "        {\"ko\": \"고마워요\", \"mt\": \"Thank you\"},\n",
    "        {\"ko\": \"지금 몇 시예요?\", \"mt\": \"What time is it now?\"},\n",
    "        {\"ko\": \"커피 좋아해요\", \"mt\": \"I like coffee\"},\n",
    "        {\"ko\": \"어디 가세요?\", \"mt\": \"Where are you going?\"},\n",
    "        {\"ko\": \"배고파요\", \"mt\": \"I am hungry\"},\n",
    "        {\"ko\": \"내일 만나요\", \"mt\": \"See you tomorrow\"},\n",
    "        {\"ko\": \"잘 자요\", \"mt\": \"Good night\"},\n",
    "        {\"ko\": \"학교에 갑니다\", \"mt\": \"I go to school\"},\n",
    "        {\"ko\": \"책을 읽습니다\", \"mt\": \"I read a book\"},\n",
    "        {\"ko\": \"음악을 듣습니다\", \"mt\": \"I listen to music\"},\n",
    "        {\"ko\": \"영화를 봅니다\", \"mt\": \"I watch a movie\"},\n",
    "        {\"ko\": \"운동을 합니다\", \"mt\": \"I exercise\"},\n",
    "        {\"ko\": \"요리를 합니다\", \"mt\": \"I cook\"},\n",
    "        {\"ko\": \"친구를 만납니다\", \"mt\": \"I meet friends\"},\n",
    "        {\"ko\": \"쇼핑을 합니다\", \"mt\": \"I go shopping\"},\n",
    "        {\"ko\": \"여행을 갑니다\", \"mt\": \"I go traveling\"},\n",
    "        {\"ko\": \"공부를 합니다\", \"mt\": \"I study\"}\n",
    "    ]\n",
    "    # 15 train, 5 valid\n",
    "    train = toy_pairs[:15]\n",
    "    valid = toy_pairs[15:]\n",
    "    Path(\"data\").mkdir(exist_ok=True)\n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train, f, ensure_ascii=False, indent=2)\n",
    "    with open(valid_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(valid, f, ensure_ascii=False, indent=2)\n",
    "    return train, valid\n",
    "\n",
    "train_pairs, valid_pairs = ensure_data(CONFIG[\"train_json\"], CONFIG[\"valid_json\"])\n",
    "len(train_pairs), len(valid_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec631ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SIZE CHECK ===\n",
      "Train path: 일상생활및구어체_한영/일상생활및구어체_한영_train_set.json\n",
      "Valid path: 일상생활및구어체_한영/일상생활및구어체_한영_valid_set.json\n",
      "Train file size: 927.6 MB\n",
      "Valid file size: 115.9 MB\n",
      "Train samples: 1\n",
      "Error reading train data: 0\n",
      "Valid samples: 1\n",
      "Error reading valid data: 0\n"
     ]
    }
   ],
   "source": [
    "# 데이터 크기 확인\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIG에서 경로 가져오기 (fallback 포함)\n",
    "train_path = CONFIG.get(\"train_json\", \"일상생활및구어체_한영/일상생활및구어체_한영_train_set.json\")\n",
    "valid_path = CONFIG.get(\"valid_json\", \"일상생활및구어체_한영/일상생활및구어체_한영_valid_set.json\")\n",
    "\n",
    "print(\"=== DATA SIZE CHECK ===\")\n",
    "print(f\"Train path: {train_path}\")\n",
    "print(f\"Valid path: {valid_path}\")\n",
    "\n",
    "# 파일 존재 확인\n",
    "if Path(train_path).exists():\n",
    "    print(f\"Train file size: {Path(train_path).stat().st_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"Train file not found!\")\n",
    "\n",
    "if Path(valid_path).exists():\n",
    "    print(f\"Valid file size: {Path(valid_path).stat().st_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"Valid file not found!\")\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "try:\n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "        print(f\"Train samples: {len(train_data)}\")\n",
    "        if train_data and len(train_data) > 0:\n",
    "            print(f\"Sample data: {train_data[0]}\")\n",
    "        else:\n",
    "            print(\"Train data is empty\")\n",
    "except (FileNotFoundError, json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "    print(f\"Error reading train data: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(valid_path, 'r', encoding='utf-8') as f:\n",
    "        valid_data = json.load(f)\n",
    "        print(f\"Valid samples: {len(valid_data)}\")\n",
    "        if valid_data and len(valid_data) > 0:\n",
    "            print(f\"Sample data: {valid_data[0]}\")\n",
    "        else:\n",
    "            print(\"Valid data is empty\")\n",
    "except (FileNotFoundError, json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "    print(f\"Error reading valid data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf42ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 샘플링 함수\n",
    "def sample_data(data, sample_size=1000, random_seed=42):\n",
    "    \"\"\"데이터에서 지정된 크기만큼 랜덤 샘플링\"\"\"\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    if len(data) <= sample_size:\n",
    "        return data\n",
    "    \n",
    "    sampled = random.sample(data, sample_size)\n",
    "    print(f\"Sampled {len(sampled)} from {len(data)} total samples\")\n",
    "    return sampled\n",
    "\n",
    "# 다양한 크기로 샘플링\n",
    "SAMPLE_SIZES = [100, 500, 1000, 2000]  # 테스트용 크기들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5f03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Tokenizer (SentencePiece or Whitespace)\n",
    "# =====================\n",
    "SPECIAL_TOKENS = {\"UNK\":0, \"BOS\":1, \"EOS\":2, \"PAD\":3}\n",
    "UNK, BOS, EOS, PAD = SPECIAL_TOKENS[\"UNK\"], SPECIAL_TOKENS[\"BOS\"], SPECIAL_TOKENS[\"EOS\"], SPECIAL_TOKENS[\"PAD\"]\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    \"\"\"간단 공백 기반 토크나이저 + vocab 빌더 (SentencePiece 폴백용)\"\"\"\n",
    "    def __init__(self, texts, vocab_size=8000):\n",
    "        print(f\"[INFO] Building WhitespaceTokenizer with {len(texts)} texts...\")\n",
    "        # 공백 토큰 수집\n",
    "        freq = Counter()\n",
    "        for t in texts:\n",
    "            tokens = basic_clean(t).split()\n",
    "            freq.update(tokens)\n",
    "        # 빈도 상위 vocab_size-4\n",
    "        most = [w for w,_ in freq.most_common(max(0, vocab_size-4))]\n",
    "        # 사전\n",
    "        self.itos = [\"<unk>\", \"<bos>\", \"<eos>\", \"<pad>\"] + most\n",
    "        self.stoi = {w:i for i,w in enumerate(self.itos)}\n",
    "        print(f\"[INFO] WhitespaceTokenizer built: {len(self.itos)} tokens\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        toks = basic_clean(text).split()\n",
    "        return [self.stoi.get(t, UNK) for t in toks]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # bos/eos/pad 제거\n",
    "        out = []\n",
    "        for i in ids:\n",
    "            if i in (BOS, EOS, PAD):\n",
    "                continue\n",
    "            out.append(self.itos[i] if 0 <= i < len(self.itos) else \"<unk>\")\n",
    "        return \" \".join(out)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, corpus_path, model_prefix, vocab_size=8000, coverage=0.9995):\n",
    "        print(f\"[INFO] Building SentencePieceTokenizer: {model_prefix}\")\n",
    "        # 모델이 없으면 학습\n",
    "        if not Path(model_prefix+\".model\").exists():\n",
    "            print(f\"[INFO] Training new SentencePiece model: {model_prefix}\")\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=corpus_path, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                model_type=\"unigram\", character_coverage=coverage,\n",
    "                unk_id=UNK, bos_id=BOS, eos_id=EOS, pad_id=PAD\n",
    "            )\n",
    "            print(f\"[INFO] SentencePiece model trained: {model_prefix}\")\n",
    "        else:\n",
    "            print(f\"[INFO] Loading existing SentencePiece model: {model_prefix}\")\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(model_prefix + \".model\")\n",
    "        print(f\"[INFO] SentencePieceTokenizer loaded: {model_prefix}\")\n",
    "        print(f\"[INFO] SentencePieceTokenizer ready: {self.sp.get_piece_size()} tokens\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return list(self.sp.encode(text, out_type=int))\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # spm은 bos/eos/pad를 무시하고 decode\n",
    "        return self.sp.decode(ids)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return self.sp.get_piece_size()\n",
    "\n",
    "def build_tokenizers(pairs_train, pairs_valid):\n",
    "    \"\"\"ko/en 각각 토크나이저 구축. SentencePiece 있으면 사용, 없으면 공백기반.\"\"\"\n",
    "    print(\"[INFO] Building tokenizers...\")\n",
    "    # Check if pairs_train is a list of dictionaries or just strings\n",
    "    if isinstance(pairs_train, list) and pairs_train:\n",
    "        if isinstance(pairs_train[0], dict):\n",
    "            all_ko = [basic_clean(x[\"ko\"]) for x in pairs_train]\n",
    "            all_en = [basic_clean(x[\"mt\"]) for x in pairs_train]\n",
    "        else:\n",
    "            # If it's a list of strings, handle differently\n",
    "            print(f\"[DEBUG] pairs_train type: {type(pairs_train)}, first item: {pairs_train[0] if pairs_train else 'empty'}\")\n",
    "            raise ValueError(\"Expected pairs_train to be a list of dictionaries with 'ko' and 'mt' keys\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] pairs_train type: {type(pairs_train)}, content: {pairs_train}\")\n",
    "        raise ValueError(\"Expected pairs_train to be a non-empty list\")\n",
    "    \n",
    "    print(f\"[INFO] Korean texts: {len(all_ko)}, English texts: {len(all_en)}\")\n",
    "\n",
    "    Path(\"spm\").mkdir(exist_ok=True)\n",
    "    \n",
    "    if CONFIG[\"use_sentencepiece\"] and HAS_SPM:\n",
    "        print(\"[INFO] Using SentencePiece tokenizers...\")\n",
    "        # 학습에 사용할 말뭉치 저장\n",
    "        ko_corpus, en_corpus = Path(\"spm/corpus.ko.txt\"), Path(\"spm/corpus.en.txt\")\n",
    "        print(f\"[INFO] Writing Korean corpus to {ko_corpus}\")\n",
    "        with open(ko_corpus, \"w\", encoding=\"utf-8\") as f:\n",
    "            for s in all_ko: f.write(s + \"\\n\")\n",
    "        print(f\"[INFO] Writing English corpus to {en_corpus}\")\n",
    "        with open(en_corpus, \"w\", encoding=\"utf-8\") as f:\n",
    "            for s in all_en: f.write(s + \"\\n\")\n",
    "        print(f\"[INFO] Building SentencePieceTokenizer for Korean: spm/ko\")\n",
    "        tok_ko = SentencePieceTokenizer(str(ko_corpus), \"spm/ko\", CONFIG[\"spm_vocab_ko\"], coverage=0.9995)\n",
    "        print(f\"[INFO] Building SentencePieceTokenizer for English: spm/en\")\n",
    "        tok_en = SentencePieceTokenizer(str(en_corpus), \"spm/en\", CONFIG[\"spm_vocab_en\"], coverage=1.0)\n",
    "        mode = \"SentencePiece\"\n",
    "    else:\n",
    "        print(\"[INFO] Using Whitespace tokenizers...\")\n",
    "        tok_ko = WhitespaceTokenizer(all_ko, vocab_size=CONFIG[\"spm_vocab_ko\"])\n",
    "        tok_en = WhitespaceTokenizer(all_en, vocab_size=CONFIG[\"spm_vocab_en\"])\n",
    "        mode = \"WhitespaceTokenizer\"\n",
    "    print(f\"[Tokenizer] mode={mode}, koV={tok_ko.vocab_size()}, enV={tok_en.vocab_size()}\")\n",
    "    return tok_ko, tok_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138569d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# CONFIG 수정\\nCONFIG[\"use_sentencepiece\"] = False\\nprint(\"Switched to WhitespaceTokenizer\")\\n\\n# 토크나이저 재빌드\\ntok_ko, tok_en = build_tokenizers(train_pairs, valid_pairs)\\nprint(\"Tokenizers ready!\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# CONFIG 수정\n",
    "CONFIG[\"use_sentencepiece\"] = False\n",
    "print(\"Switched to WhitespaceTokenizer\")\n",
    "\n",
    "# 토크나이저 재빌드\n",
    "tok_ko, tok_en = build_tokenizers(train_pairs, valid_pairs)\n",
    "print(\"Tokenizers ready!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ad0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"[INFO] Building tokenizers...\")\n",
    "tok_ko, tok_en = build_tokenizers(train_pairs, valid_pairs)\n",
    "print(\"[INFO] Tokenizers ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Dataset & Collate\n",
    "# =====================\n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs, tok_ko, tok_en, src_max, tgt_max):\n",
    "        self.pairs = pairs\n",
    "        self.tok_ko = tok_ko\n",
    "        self.tok_en = tok_en\n",
    "        self.src_max = src_max\n",
    "        self.tgt_max = tgt_max\n",
    "    \n",
    "    def __len__(self): return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        ko = basic_clean(self.pairs[i][\"ko\"])\n",
    "        en = basic_clean(self.pairs[i][\"mt\"])\n",
    "        # encode\n",
    "        src_ids = [BOS] + self.tok_ko.encode(ko) + [EOS]\n",
    "        tgt_ids = [BOS] + self.tok_en.encode(en) + [EOS]\n",
    "        # truncate\n",
    "        src_ids = src_ids[:self.src_max]\n",
    "        tgt_ids = tgt_ids[:self.tgt_max]\n",
    "        # raw lengths (BOS/EOS 제외한 토큰 길이)\n",
    "        ko_raw = max(0, len(src_ids)-2)\n",
    "        en_raw = max(0, len(tgt_ids)-2)\n",
    "        return {\n",
    "            \"src\": torch.tensor(src_ids, dtype=torch.long),\n",
    "            \"tgt\": torch.tensor(tgt_ids, dtype=torch.long),\n",
    "            \"ko_raw\": ko_raw,\n",
    "            \"en_raw\": en_raw,\n",
    "        }\n",
    "\n",
    "def pad_sequences(seqs, pad=PAD):\n",
    "    maxlen = max(s.size(0) for s in seqs)\n",
    "    out = torch.full((len(seqs), maxlen), pad, dtype=torch.long)\n",
    "    lens = []\n",
    "    for i, s in enumerate(seqs):\n",
    "        out[i, :s.size(0)] = s\n",
    "        lens.append(int(s.size(0)))\n",
    "    return out, torch.tensor(lens, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    srcs = [b[\"src\"] for b in batch]\n",
    "    tgts = [b[\"tgt\"] for b in batch]\n",
    "    src, _ = pad_sequences(srcs, PAD)\n",
    "    tgt, _ = pad_sequences(tgts, PAD)\n",
    "    # 효과적 길이: raw+2, 패딩 길이로 clamp\n",
    "    ko_lengths = torch.clamp(torch.tensor([b[\"ko_raw\"] for b in batch]) + 2, max=src.size(1))\n",
    "    en_lengths = torch.clamp(torch.tensor([b[\"en_raw\"] for b in batch]) + 2, max=tgt.size(1))\n",
    "    # 디코더용 in/out 분리\n",
    "    tgt_in  = tgt[:, :-1]\n",
    "    tgt_out = tgt[:, 1:]\n",
    "    return src, ko_lengths, tgt_in, tgt_out\n",
    "\n",
    "train_ds = NMTDataset(train_pairs, tok_ko, tok_en, CONFIG[\"src_max_len\"], CONFIG[\"tgt_max_len\"])\n",
    "valid_ds = NMTDataset(valid_pairs, tok_ko, tok_en, CONFIG[\"src_max_len\"], CONFIG[\"tgt_max_len\"])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "print(\"sanity shapes:\", [x.shape if isinstance(x, torch.Tensor) else type(x) for x in batch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Models: Encoder (BiGRU), Attention, Decoder\n",
    "# =====================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, emb, hid):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
    "        self.gru = nn.GRU(emb, hid, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(hid*2, hid)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # lengths: clamp + cpu\n",
    "        if not isinstance(lengths, torch.Tensor):\n",
    "            lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        lengths = lengths.clamp(min=1, max=x.size(1)).cpu()\n",
    "        emb = self.drop(self.emb(x))\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=x.size(1))  # [B,S,2H]\n",
    "        h_cat = torch.cat([h[-2], h[-1]], dim=-1)  # [B,2H]\n",
    "        h0 = torch.tanh(self.proj(h_cat)).unsqueeze(0)  # [1,B,H]\n",
    "        return out, h0\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, dec_hid, enc_dim, attn_dim=256):\n",
    "        super().__init__()\n",
    "        self.W_h = nn.Linear(dec_hid, attn_dim, bias=False)\n",
    "        self.W_e = nn.Linear(enc_dim, attn_dim, bias=False)\n",
    "        self.v   = nn.Linear(attn_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, dec_h, enc_out, src_mask):\n",
    "        # dec_h: [B,H], enc_out: [B,S,EncDim], src_mask: [B,S] (True for keep)\n",
    "        q = self.W_h(dec_h).unsqueeze(1)         # [B,1,A]\n",
    "        k = self.W_e(enc_out)                    # [B,S,A]\n",
    "        e = self.v(torch.tanh(q + k)).squeeze(-1)  # [B,S]\n",
    "        e = e.masked_fill(~src_mask, float(\"-inf\"))\n",
    "        a = torch.softmax(e, dim=-1)             # [B,S]\n",
    "        ctx = torch.bmm(a.unsqueeze(1), enc_out).squeeze(1)  # [B,EncDim]\n",
    "        return ctx, a\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, emb, hid):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
    "        self.gru = nn.GRU(emb, hid, batch_first=True)\n",
    "        self.out = nn.Linear(hid, vocab)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, y_in, h0):\n",
    "        emb = self.drop(self.emb(y_in))       # [B,T,E]\n",
    "        out, h = self.gru(emb, h0)            # [B,T,H]\n",
    "        logits = self.out(out)                # [B,T,V]\n",
    "        return logits, h\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, vocab, emb, hid, enc_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
    "        self.gru = nn.GRU(emb + enc_dim, hid, batch_first=True)\n",
    "        self.out = nn.Linear(hid, vocab)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.attn = AdditiveAttention(hid, enc_dim)\n",
    "    \n",
    "    def forward(self, y_in, h0, enc_out, src_mask):\n",
    "        B, T = y_in.size()\n",
    "        h = h0\n",
    "        logits = []\n",
    "        for t in range(T):\n",
    "            emb_t = self.drop(self.emb(y_in[:, t:t+1]))  # [B,1,E]\n",
    "            dec_h = h[-1]                                 # [B,H]\n",
    "            ctx, _ = self.attn(dec_h, enc_out, src_mask)  # [B,EncDim]\n",
    "            rnn_in = torch.cat([emb_t.squeeze(1), ctx], dim=-1).unsqueeze(1)  # [B,1,E+EncDim]\n",
    "            out, h = self.gru(rnn_in, h)\n",
    "            logits.append(self.out(out))  # [B,1,V]\n",
    "        logits = torch.cat(logits, dim=1)  # [B,T,V]\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "    def forward(self, src, src_len, tgt_in):\n",
    "        enc_out, h0 = self.enc(src, src_len)\n",
    "        logits, _ = self.dec(tgt_in, h0)\n",
    "        return logits\n",
    "\n",
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "    def forward(self, src, src_len, tgt_in):\n",
    "        enc_out, h0 = self.enc(src, src_len)\n",
    "        src_mask = (src != PAD)\n",
    "        logits, _ = self.dec(tgt_in, h0, enc_out, src_mask)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Training / Validation / Decode / BLEU\n",
    "# =====================\n",
    "from math import exp\n",
    "\n",
    "def linear_tf_ratio(epoch, max_epoch, start=1.0, end=0.5):\n",
    "    if max_epoch <= 1: return end\n",
    "    t = epoch / (max_epoch - 1)\n",
    "    return float(start + (end - start)*t)\n",
    "\n",
    "def train_epoch(model, dl, opt, criterion, device=\"cpu\", teacher_forcing=1.0):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for src, src_len, tgt_in, tgt_out in dl:\n",
    "        src, src_len, tgt_in, tgt_out = src.to(device), src_len, tgt_in.to(device), tgt_out.to(device)\n",
    "        \n",
    "        # Teacher Forcing 적용\n",
    "        if random.random() < teacher_forcing:\n",
    "            # Teacher forcing: 정답 입력 사용\n",
    "            logits = model(src, src_len, tgt_in)\n",
    "        else:\n",
    "            # No teacher forcing: 모델 예측 사용\n",
    "            # (이 부분은 더 복잡한 구현 필요)\n",
    "            logits = model(src, src_len, tgt_in)\n",
    "        \n",
    "        # 타깃 시프트: collate에서 이미 in/out 분리 완료\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    return total/len(dl)\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_epoch(model, dl, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for src, src_len, tgt_in, tgt_out in dl:\n",
    "        src, src_len, tgt_in, tgt_out = src.to(device), src_len, tgt_in.to(device), tgt_out.to(device)\n",
    "        logits = model(src, src_len, tgt_in)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        total += loss.item()\n",
    "    ppl = float(np.exp(total/len(dl)))\n",
    "    return total/len(dl), ppl\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, src, sp_tgt, max_len=64, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    src_len = torch.tensor([src.size(1)], dtype=torch.long)\n",
    "    # 인코드\n",
    "    if isinstance(model, Seq2Seq):\n",
    "        enc_out, h0 = model.enc(src, src_len)\n",
    "        y = torch.tensor([[BOS]], device=device)\n",
    "        h = h0\n",
    "        out_ids = []\n",
    "        for _ in range(max_len):\n",
    "            logits, h = model.dec(y, h)  # [1,T,V]\n",
    "            next_id = int(logits[:, -1, :].argmax(-1))\n",
    "            if next_id == EOS: break\n",
    "            out_ids.append(next_id)\n",
    "            y = torch.cat([y, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "        return sp_tgt.decode(out_ids)\n",
    "    else:\n",
    "        enc_out, h0 = model.enc(src, src_len)\n",
    "        src_mask = (src != PAD)\n",
    "        y = torch.tensor([[BOS]], device=device)\n",
    "        h = h0\n",
    "        out_ids = []\n",
    "        for _ in range(max_len):\n",
    "            # 한 스텝\n",
    "            emb = model.dec.emb(y[:, -1:])   # [1,1,E]\n",
    "            dec_h = h[-1]                    # [1,H]\n",
    "            ctx, _ = model.dec.attn(dec_h, enc_out, src_mask)\n",
    "            rnn_in = torch.cat([emb.squeeze(1), ctx], dim=-1).unsqueeze(1)\n",
    "            o, h = model.dec.gru(rnn_in, h)\n",
    "            logit = model.dec.out(o)         # [1,1,V]\n",
    "            nxt = int(logit[:, -1, :].argmax(-1))\n",
    "            if nxt == EOS: break\n",
    "            out_ids.append(nxt)\n",
    "            y = torch.cat([y, torch.tensor([[nxt]], device=device)], dim=1)\n",
    "        return sp_tgt.decode(out_ids)\n",
    "\n",
    "# BLEU: sacrebleu 있으면 사용, 없으면 매우 간단한 대체(유니그램 BLEU 비슷)\n",
    "def simple_bleu(hyps, refs):\n",
    "    # 아주 간단: 유니그램 precision * brevity penalty\n",
    "    def prec(h, r):\n",
    "        ht = h.split()\n",
    "        rt = r.split()\n",
    "        if not ht: return 0.0\n",
    "        count_h = Counter(ht)\n",
    "        count_r = Counter(rt)\n",
    "        overlap = sum(min(count_h[w], count_r[w]) for w in count_h)\n",
    "        return overlap / len(ht)\n",
    "    def bp(h, r):\n",
    "        len_h = len(h.split())\n",
    "        len_r = len(r.split())\n",
    "        if len_h==0: return 0.0\n",
    "        return 1.0 if len_h>len_r else math.exp(1 - len_r/len_h) if len_h>0 else 0.0\n",
    "    \n",
    "    scores = []\n",
    "    for h, r in zip(hyps, refs):\n",
    "        scores.append(100.0 * prec(h, r) * bp(h, r))\n",
    "    return sum(scores)/len(scores) if scores else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_bleu(model, ds, tok_src, tok_tgt, n_samples=200, device=\"cpu\"):\n",
    "    n = min(n_samples, len(ds))\n",
    "    hyps, refs = [], []\n",
    "    for i in range(n):\n",
    "        item = ds[i]\n",
    "        src = item[\"src\"].unsqueeze(0)  # [1,S]\n",
    "        hyp = greedy_decode(model, src, tok_tgt, max_len=CONFIG[\"tgt_max_len\"], device=device)\n",
    "        ref = tok_tgt.decode(item[\"tgt\"].tolist())\n",
    "        hyps.append(hyp.strip())\n",
    "        refs.append(ref.strip())\n",
    "    if HAS_SACREBLEU:\n",
    "        return sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    else:\n",
    "        return simple_bleu(hyps, refs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Train & Evaluate\n",
    "# =====================\n",
    "device = CONFIG[\"device\"]\n",
    "\n",
    "SRC_V = tok_ko.vocab_size()\n",
    "TGT_V = tok_en.vocab_size()\n",
    "\n",
    "enc = Encoder(SRC_V, CONFIG[\"emb\"], CONFIG[\"enc_hid\"])\n",
    "dec_base = Decoder(TGT_V, CONFIG[\"emb\"], CONFIG[\"dec_hid\"])\n",
    "model_base = Seq2Seq(enc, dec_base).to(device)\n",
    "\n",
    "encA = Encoder(SRC_V, CONFIG[\"emb\"], CONFIG[\"enc_hid\"])\n",
    "dec_attn = AttnDecoder(TGT_V, CONFIG[\"emb\"], CONFIG[\"dec_hid\"], enc_dim=CONFIG[\"enc_hid\"]*2)\n",
    "model_attn = Seq2SeqAttn(encA, dec_attn).to(device) if CONFIG[\"use_attention\"] else None\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "opt_base = torch.optim.Adam(model_base.parameters(), lr=CONFIG[\"lr\"])\n",
    "opt_attn = torch.optim.Adam(model_attn.parameters(), lr=CONFIG[\"lr\"]) if model_attn else None\n",
    "\n",
    "# Train baseline\n",
    "print(\"== Train: Seq2Seq (baseline) ==\")\n",
    "for e in range(CONFIG[\"epochs\"]):\n",
    "    tf = linear_tf_ratio(e, CONFIG[\"epochs\"], CONFIG[\"teacher_forcing_start\"], CONFIG[\"teacher_forcing_end\"])\n",
    "    tr = train_epoch(model_base, train_dl, opt_base, criterion, device=device, teacher_forcing=tf)\n",
    "    va, ppl = valid_epoch(model_base, valid_dl, criterion, device=device)\n",
    "    print(f\"[BASE] ep{e+1}/{CONFIG['epochs']}  train {tr:.3f}  valid {va:.3f}  ppl {ppl:.2f}\")\n",
    "\n",
    "bleu_b = eval_bleu(model_base, valid_ds, tok_ko, tok_en, n_samples=min(200, len(valid_ds)), device=device)\n",
    "print(f\"[BASE] BLEU(valid): {bleu_b:.2f}\")\n",
    "\n",
    "# Train attention (optional)\n",
    "if model_attn:\n",
    "    print(\"\\n== Train: Seq2Seq + Bahdanau Attention ==\")\n",
    "    for e in range(CONFIG[\"epochs\"]):\n",
    "        tf = linear_tf_ratio(e, CONFIG[\"epochs\"], CONFIG[\"teacher_forcing_start\"], CONFIG[\"teacher_forcing_end\"])\n",
    "        tr = train_epoch(model_attn, train_dl, opt_attn, criterion, device=device, teacher_forcing=tf)\n",
    "        va, ppl = valid_epoch(model_attn, valid_dl, criterion, device=device)\n",
    "        print(f\"[ATTN] ep{e+1}/{CONFIG['epochs']}  train {tr:.3f}  valid {va:.3f}  ppl {ppl:.2f}\")\n",
    "    bleu_a = eval_bleu(model_attn, valid_ds, tok_ko, tok_en, n_samples=min(200, len(valid_ds)), device=device)\n",
    "    print(f\"[ATTN] BLEU(valid): {bleu_a:.2f}\")\n",
    "else:\n",
    "    print(\"[INFO] Attention 모델은 비활성화됨 (CONFIG['use_attention']=False).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Sample Translations\n",
    "# =====================\n",
    "@torch.no_grad()\n",
    "def show_samples(model, ds, tok_src, tok_tgt, k=5, device=\"cpu\"):\n",
    "    print(\"----- Sample translations -----\")\n",
    "    for i in range(min(k, len(ds))):\n",
    "        item = ds[i]\n",
    "        src_text = basic_clean(train_pairs[i][\"ko\"]) if i < len(train_pairs) else \"<src>\"\n",
    "        ref_text = basic_clean(train_pairs[i][\"mt\"]) if i < len(train_pairs) else \"<ref>\"\n",
    "        hyp = greedy_decode(model, item[\"src\"].unsqueeze(0), tok_tgt, max_len=CONFIG[\"tgt_max_len\"], device=device)\n",
    "        print(f\"KO: {src_text}\")\n",
    "        print(f\"REF: {ref_text}\")\n",
    "        print(f\"HYP: {hyp}\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "print(\"\\n[Samples: baseline]\")\n",
    "show_samples(model_base, valid_ds, tok_ko, tok_en, k=min(5, len(valid_ds)), device=device)\n",
    "\n",
    "if model_attn:\n",
    "    print(\"\\n[Samples: attention]\")\n",
    "    show_samples(model_attn, valid_ds, tok_ko, tok_en, k=min(5, len(valid_ds)), device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5027a7d",
   "metadata": {},
   "source": [
    "\n",
    "### 저장/체크포인트(옵션)\n",
    "- 슬림 버전에서는 기본 **OFF**입니다. 필요 시 다음을 참고:\n",
    "```python\n",
    "if CONFIG[\"use_checkpoint\"]:\n",
    "    torch.save(model_base.state_dict(), \"model_base.pt\")\n",
    "    if CONFIG[\"use_attention\"]:\n",
    "        torch.save(model_attn.state_dict(), \"model_attn.pt\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_3 (conda)",
   "language": "python",
   "name": "ai_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
