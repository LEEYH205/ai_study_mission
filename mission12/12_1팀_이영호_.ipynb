{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d338e81",
   "metadata": {},
   "source": [
    "# 미션 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f626407",
   "metadata": {},
   "source": [
    "Hugging Face transformers 라이브러리를 사용하여 문서 요약 모델을 구현하는 미션.\n",
    "데이터 로드 및 전처리부터 모델 실행, 결과 평가까지 전체 파이프라인을 구축."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83904421",
   "metadata": {},
   "source": [
    "## 사용 데이터셋\n",
    "- 데이터 형식\n",
    "    - JSON 파일 형태로 제공되며, 3종류(신문 기사, 사설, 법률)의 문서가 포함되어 있다.\n",
    "- 데이터 구성\n",
    "    - 각 문서 타입은 train/test 쌍으로 구성되어 있으며, 전체 데이터를 모두 사용하거나 원하는 문서 종류를 선택하여 학습시키면 된다.\n",
    "\n",
    "## 가이드라인\n",
    "- 데이터 로드 및 전처리\n",
    "    - 문서 데이터를 로드하고, 불필요한 기호나 공백을 제거하는 등 전처리 작업을 수행\n",
    "    - 텍스트 길이를 확인하고, 모델 입력에 적합한 형식으로 변환한다.\n",
    "- 모델 선택 및 실행\n",
    "    - Hugging Face의 Transformers 라이브러리를 활용해 문서 요약을 수행\n",
    "    - 사전 학습된 모델을 활용하거나 주어진 데이터를 가지고 Fine-tuning 하기.\n",
    "- 모델 평가 및 결과 분석\n",
    "    - 생성된 요약문과 원본 문서를 비교하여 ROUGE 등의 평가 지표를 사용해 요약 품질을 분석한다.\n",
    "    - 테스트 문장에 대한 요약 결과를 출력하여 모델의 성능을 확인한다.\n",
    "- 모델 구현 및 학습 결과\n",
    "    - 문서 요약 모델(예: Transformer 기반 요약 모델, T5, BART 등)을 구현하고, 데이터 로드 → 전처리 → 모델 구축 및 학습 → 요약 생성 및 평가 과정을 순차적으로 진행.\n",
    "- 모델 성능 평가 및 제출\n",
    "    - 생성된 요약문의 품질을 정성적(요약 결과 확인) 및 정량적(ROUGE 등)으로 평가.\n",
    "    - 테스트 데이터셋에 대한 요약 결과를 포함\n",
    "- 원본 데이터셋 링크\n",
    "    - https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3cf7b",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (4.55.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [datasets]4/5\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.0.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Requirement already satisfied: konlpy in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb10b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from konlpy.tag import Okt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196d8af",
   "metadata": {},
   "source": [
    "## GPU 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0\n",
      "MPS available: True\n",
      "CUDA available: False\n",
      "Using MPS (Apple Silicon GPU)\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # 맥북 M1/M2 GPU\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU (Colab, Windows 등)\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a998bca",
   "metadata": {},
   "source": [
    "# KoBART 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe02291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gogamza/kobart-base-v2 모델 다운로드 중 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토크나이저 다운로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 다운로드 완료!\n",
      "\n",
      "�� 모델 정보:\n",
      "- 토크나이저 타입: PreTrainedTokenizerFast\n",
      "- 모델 타입: BartForConditionalGeneration\n",
      "- 어휘 크기: 30,000\n",
      "- 모델 파라미터: 123,859,968\n",
      "\n",
      "�� 모델이 mps 디바이스로 이동되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# KoBART 모델 다운로드\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"=== gogamza/kobart-base-v2 모델 다운로드 중 ===\")\n",
    "\n",
    "# 토크나이저 다운로드 (models 폴더에 저장)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2', cache_dir='./models')\n",
    "print(\"✅ 토크나이저 다운로드 완료!\")\n",
    "\n",
    "# 모델 다운로드 (models 폴더에 저장)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('gogamza/kobart-base-v2', cache_dir='./models')\n",
    "print(\"✅ 모델 다운로드 완료!\")\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"\\n�� 모델 정보:\")\n",
    "print(f\"- 토크나이저 타입: {type(tokenizer).__name__}\")\n",
    "print(f\"- 모델 타입: {type(model).__name__}\")\n",
    "print(f\"- 어휘 크기: {tokenizer.vocab_size:,}\")\n",
    "print(f\"- 모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 모델을 디바이스로 이동\n",
    "model = model.to(device)\n",
    "print(f\"\\n�� 모델이 {device} 디바이스로 이동되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c453213",
   "metadata": {},
   "source": [
    "# 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf6d7c",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1baf49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 전처리 함수\n",
    "def load_json_dataset(file_path):\n",
    "    \"\"\"JSON 파일을 로드하여 문서별 text, summary 정보를 추출\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    examples = []\n",
    "    for doc in data[\"documents\"]:\n",
    "        sentences = []\n",
    "        # \"text\"는 중첩 리스트 형태이므로 내부의 모든 sentence를 추출\n",
    "        for sublist in doc[\"text\"]:\n",
    "            for item in sublist:\n",
    "                sentences.append(item.get(\"sentence\", \"\"))\n",
    "        \n",
    "        full_text = \" \".join(sentences)\n",
    "        # abstractive 요약은 첫번째 항목 사용 (없으면 빈 문자열)\n",
    "        summary = doc[\"abstractive\"][0] if doc[\"abstractive\"] else \"\"\n",
    "        \n",
    "        examples.append({\n",
    "            \"text\": full_text,\n",
    "            \"summary\": summary,\n",
    "        })\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c7360",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a109f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 로드 중 ===\n",
      "훈련 데이터: 24,329개\n",
      "검증 데이터: 3,004개\n",
      "\n",
      "=== 샘플 데이터 ===\n",
      "첫 번째 훈련 예시:\n",
      "텍스트 길이: 372자\n",
      "요약 길이: 97자\n",
      "텍스트 미리보기: 원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해태하고, 노조조합장이 사임한 경우, 노동조합규약에 동 조합장의 직무를 대행할 자를 규정해 두고 있...\n",
      "요약: 원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 한다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 경로 설정\n",
    "base_path = \"./summarization/\"\n",
    "\n",
    "# 처음에는 작은 법률 데이터셋으로 시작\n",
    "train_file = \"train_original_law.json\"\n",
    "valid_file = \"valid_original_law.json\"\n",
    "\n",
    "print(\"=== 데이터 로드 중 ===\")\n",
    "train_examples = load_json_dataset(os.path.join(base_path, train_file))\n",
    "valid_examples = load_json_dataset(os.path.join(base_path, valid_file))\n",
    "\n",
    "print(f\"훈련 데이터: {len(train_examples):,}개\")\n",
    "print(f\"검증 데이터: {len(valid_examples):,}개\")\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "print(f\"\\n=== 샘플 데이터 ===\")\n",
    "print(f\"첫 번째 훈련 예시:\")\n",
    "print(f\"텍스트 길이: {len(train_examples[0]['text'])}자\")\n",
    "print(f\"요약 길이: {len(train_examples[0]['summary'])}자\")\n",
    "print(f\"텍스트 미리보기: {train_examples[0]['text'][:100]}...\")\n",
    "print(f\"요약: {train_examples[0]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40499dbb",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dad1518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 전처리 중 ===\n",
      "전처리 후 훈련 데이터: 22,514개\n",
      "전처리 후 검증 데이터: 2,836개\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_data(examples, max_text_length=512, max_summary_length=128):\n",
    "    \"\"\"텍스트와 요약을 전처리하고 길이 제한\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for example in examples:\n",
    "        text = example[\"text\"].strip()\n",
    "        summary = example[\"summary\"].strip()\n",
    "        \n",
    "        # 빈 요약 제거\n",
    "        if not summary:\n",
    "            continue\n",
    "            \n",
    "        # 길이 제한\n",
    "        if len(text) > max_text_length * 3:  # 대략적인 토큰 수 추정\n",
    "            continue\n",
    "        if len(summary) > max_summary_length * 3:\n",
    "            continue\n",
    "            \n",
    "        processed.append({\n",
    "            \"text\": text,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# 데이터 전처리 적용\n",
    "print(\"=== 데이터 전처리 중 ===\")\n",
    "train_processed = preprocess_data(train_examples)\n",
    "valid_processed = preprocess_data(valid_examples)\n",
    "\n",
    "print(f\"전처리 후 훈련 데이터: {len(train_processed):,}개\")\n",
    "print(f\"전처리 후 검증 데이터: {len(valid_processed):,}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d65a09",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Dataset으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2be1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset 변환 완료 ===\n",
      "데이터셋 구조: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 22514\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary'],\n",
      "        num_rows: 2836\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Dataset 객체 생성\n",
    "train_dataset = Dataset.from_list(train_processed)\n",
    "valid_dataset = Dataset.from_list(valid_processed)\n",
    "\n",
    "# DatasetDict 형태로 통합\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": valid_dataset\n",
    "})\n",
    "\n",
    "print(\"=== Dataset 변환 완료 ===\")\n",
    "print(f\"데이터셋 구조: {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b4c3a",
   "metadata": {},
   "source": [
    "## 5. tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4efee947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토크나이징 중 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1d998d5d924aeb895345a8d169c374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4745f43b2aa0405aabd489d87ec96189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토크나이징 완료!\n",
      "토크나이징된 훈련 데이터: 22514개\n",
      "토크나이징된 검증 데이터: 2836개\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징 함수\n",
    "def tokenize_function(example):\n",
    "    \"\"\"텍스트와 요약을 토큰화\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=example[\"summary\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 토크나이징 적용\n",
    "print(\"=== 토크나이징 중 ===\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"✅ 토크나이징 완료!\")\n",
    "print(f\"토크나이징된 훈련 데이터: {len(tokenized_datasets['train'])}개\")\n",
    "print(f\"토크나이징된 검증 데이터: {len(tokenized_datasets['validation'])}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a282d",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f0f22",
   "metadata": {},
   "source": [
    "## DataCollator 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8290600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DataCollator 설정 중 ===\n",
      "✅ DataCollator 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "# DataCollator 설정\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "print(\"=== DataCollator 설정 중 ===\")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"✅ DataCollator 설정 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7fb3de",
   "metadata": {},
   "source": [
    "## 학습 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6e9e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 학습 파라미터 설정 중 ===\n",
      "✅ 학습 파라미터 설정 완료!\n",
      "학습 에포크: 3\n",
      "학습률: 5e-05\n",
      "배치 크기: 4\n"
     ]
    }
   ],
   "source": [
    "# 학습 파라미터 설정\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"=== 학습 파라미터 설정 중 ===\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # 결과 저장 폴더\n",
    "    eval_strategy=\"steps\",            # 평가 전략\n",
    "    eval_steps=500,                   # 500 스텝마다 평가\n",
    "    save_strategy=\"steps\",            # 저장 전략\n",
    "    save_steps=1000,                  # 1000 스텝마다 모델 저장\n",
    "    learning_rate=5e-5,               # 학습률\n",
    "    per_device_train_batch_size=4,    # 배치 크기 (GPU 메모리에 따라 조정)\n",
    "    per_device_eval_batch_size=4,     # 평가 배치 크기\n",
    "    num_train_epochs=3,               # 학습 에포크\n",
    "    weight_decay=0.01,                # 가중치 감쇠\n",
    "    logging_dir=\"./logs\",             # 로그 저장 폴더\n",
    "    logging_steps=100,                # 100 스텝마다 로그\n",
    "    save_total_limit=3,               # 최대 3개 체크포인트만 저장\n",
    "    load_best_model_at_end=True,      # 최고 성능 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\", # 최고 성능 기준\n",
    "    greater_is_better=False,          # 손실은 낮을수록 좋음\n",
    "    report_to=\"none\",                 # wandb 등 외부 도구 사용 안함\n",
    ")\n",
    "\n",
    "print(\"✅ 학습 파라미터 설정 완료!\")\n",
    "print(f\"학습 에포크: {training_args.num_train_epochs}\")\n",
    "print(f\"학습률: {training_args.learning_rate}\")\n",
    "print(f\"배치 크기: {training_args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6c54b",
   "metadata": {},
   "source": [
    "## Trainer 설정 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480ab0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trainer 설정 중 ===\n",
      "✅ Trainer 설정 완료!\n",
      "훈련 데이터 크기: 22514\n",
      "검증 데이터 크기: 2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/xvtnn0v91cx8bl846n_qn4100000gn/T/ipykernel_87676/3876579206.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer 설정\n",
    "from transformers import Trainer\n",
    "\n",
    "print(\"=== Trainer 설정 중 ===\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                           # 모델\n",
    "    args=training_args,                    # 학습 파라미터\n",
    "    train_dataset=tokenized_datasets[\"train\"],      # 훈련 데이터\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # 검증 데이터\n",
    "    tokenizer=tokenizer,                   # 토크나이저\n",
    "    data_collator=data_collator,           # 데이터 콜레이터\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer 설정 완료!\")\n",
    "print(f\"훈련 데이터 크기: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"검증 데이터 크기: {len(tokenized_datasets['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12e88339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 모델 학습 시작!\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1454' max='16887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1454/16887 17:21 < 3:04:30, 1.39 it/s, Epoch 0.26/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.893400</td>\n",
       "      <td>0.752535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.817400</td>\n",
       "      <td>0.704269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/modeling_utils.py:3909: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 학습 완료!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m총 학습 시간: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_result.metrics[\u001b[33m'\u001b[39m\u001b[33mtrain_runtime\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m초\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ai_3/lib/python3.12/site-packages/transformers/trainer.py:2587\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2582\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2591\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "print(\"🚀 모델 학습 시작!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 학습 실행\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"✅ 학습 완료!\")\n",
    "print(f\"총 학습 시간: {train_result.metrics['train_runtime']:.2f}초\")\n",
    "print(f\"총 학습 스텝: {train_result.metrics['train_steps']}\")\n",
    "print(f\"최종 손실: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a2012",
   "metadata": {},
   "source": [
    "# 요약 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a9480",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_3 (conda)",
   "language": "python",
   "name": "ai_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
